{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Red Lines Forecasting: Final Submission\n",
        "\n",
        "**Publication-ready forecasting of AI capability threshold crossings**\n",
        "\n",
        "**Key improvements over hackathon version:**\n",
        "1. Model comparison framework (5 models with AIC/BIC selection)\n",
        "2. Proper probabilistic scoring (CRPS + calibration correction)\n",
        "3. Rolling-origin cross-validation (multiple horizons: 1, 2, 3 years)\n",
        "4. **Defensible threshold taxonomy** (policy-defined vs. compute milestones)\n",
        "5. ECI capability forecasting (R² = 0.84 vs. 0.11 from HuggingFace)\n",
        "6. Publication-quality figures with consistent KOSMOS color palette\n",
        "7. Backtest diagnostics for selected model\n",
        "\n",
        "**Critical update:** Only 2 FLOP thresholds are policy-defined (EU AI Act 10²⁵, US EO 10²⁶). \n",
        "Additional milestones (10×, 100×, ~3000× US EO) are training budget forecasts, NOT capability claims.\n",
        "\n",
        "---\n",
        "\n",
        "## Threshold Taxonomy\n",
        "\n",
        "### Policy-Defined Compute Thresholds\n",
        "These are thresholds explicitly defined in regulatory or executive policy:\n",
        "\n",
        "| Threshold | Value | Source | Function |\n",
        "|-----------|-------|--------|----------|\n",
        "| EU AI Act | 10²⁵ FLOP | Article 51(2) | Governance trigger (rebuttable presumption) |\n",
        "| US EO 14110 | 10²⁶ FLOP | Executive Order § 4.2(a)(i) | Reporting requirement |\n",
        "\n",
        "**Note:** These are governance triggers, not capability determinations.\n",
        "\n",
        "### Compute Milestone Markers (relative to US EO threshold)\n",
        "**IMPORTANT:** These are compute milestones anchored to policy thresholds, NOT capability claims.\n",
        "\n",
        "| Milestone | Value | Justification |\n",
        "|-----------|-------|---------------|\n",
        "| 10× US EO | 10²⁷ FLOP | 10× beyond reporting threshold (~2026 frontier) |\n",
        "| 100× US EO | 10²⁸ FLOP | 100× beyond reporting threshold (~2027 frontier) |\n",
        "| ~3000× US EO | 10²⁹·⁵ FLOP | Upper bound of 3-year forecast window |\n",
        "\n",
        "**Why not ASL-4/5 FLOP values?** Anthropic's RSP defines ASL levels via capability evaluations, not FLOP.\n",
        "**Why not TAI at 10²⁹·⁵?** Cotra (2020) estimates 10³²-10³⁶ FLOP, far beyond our forecast horizon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete.\n",
            "  Data dir: /Users/dev/Documents/apart/technical_ai_governance/redline_forecast/data\n",
            "  Output dir: /Users/dev/Documents/apart/technical_ai_governance/redline_forecast/output\n",
            "  Figure size: 6.25 x 4.42 inches\n",
            "  Color palette: 28 colors defined\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies if needed\n",
        "# !pip install pandas numpy scipy scikit-learn matplotlib seaborn properscoring statsmodels\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "from scipy.optimize import minimize_scalar\n",
        "from sklearn.linear_model import LinearRegression, QuantileRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from statsmodels.tsa.deterministic import DeterministicProcess\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")  # Non-interactive backend\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Dict, List, Tuple, Optional, Any, Callable\n",
        "import json\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Try to import properscoring for CRPS\n",
        "try:\n",
        "    import properscoring as ps\n",
        "    HAS_PROPERSCORING = True\n",
        "except ImportError:\n",
        "    print(\"properscoring not available - will use fallback CRPS\")\n",
        "    HAS_PROPERSCORING = False\n",
        "\n",
        "# =============================================================================\n",
        "# KOSMOS COLOR PALETTE (publication-quality visual identity)\n",
        "# =============================================================================\n",
        "COLORS = {\n",
        "    # Primary dark tones\n",
        "    'navy': '#1b1d26',\n",
        "    'charcoal': '#333746',\n",
        "    'slate': '#4a4f5c',\n",
        "    'gray': '#6b7280',\n",
        "    'light_gray': '#d1d5db',\n",
        "    \n",
        "    # Warm accent colors (for data visualization)\n",
        "    'gold': '#f59e0b',\n",
        "    'amber': '#d97706',\n",
        "    'orange': '#ea580c',\n",
        "    \n",
        "    # Cool accent colors\n",
        "    'teal': '#0d9488',\n",
        "    'cyan': '#06b6d4',\n",
        "    'blue': '#3b82f6',\n",
        "    \n",
        "    # Additional colors\n",
        "    'red': '#ef4444',\n",
        "    'green': '#22c55e',\n",
        "    'purple': '#8b5cf6',\n",
        "    \n",
        "    # Red line severity gradient (light to dark)\n",
        "    'rl_1': '#fef3c7',\n",
        "    'rl_2': '#fcd34d',\n",
        "    'rl_3': '#f59e0b',\n",
        "    'rl_4': '#d97706',\n",
        "    'rl_5': '#b45309',\n",
        "    'rl_6': '#92400e',\n",
        "    'rl_7': '#7c2d12',\n",
        "    'rl_8': '#991b1b',\n",
        "    'rl_9': '#7f1d1d',\n",
        "    \n",
        "    # Prediction interval colors\n",
        "    'pi_compute': '#3b82f6',\n",
        "    'pi_capability': '#10b981',\n",
        "    'pi_combined': '#8b5cf6',\n",
        "    \n",
        "    # Background/grid\n",
        "    'bg': '#fafafa',\n",
        "    'grid': '#e5e7eb',\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# PLOT STYLE SETUP (Publication-quality)\n",
        "# =============================================================================\n",
        "def setup_plot_style():\n",
        "    \"\"\"Set up consistent matplotlib styling for publication-quality figures.\"\"\"\n",
        "    \n",
        "    # DIN A aspect ratio dimensions\n",
        "    width = 6.25\n",
        "    width2 = 3.06\n",
        "    aspect_ratio = np.sqrt(2)\n",
        "    height = width / aspect_ratio\n",
        "    height2 = width2 / aspect_ratio\n",
        "    \n",
        "    # Comprehensive rcParams update\n",
        "    plt.rcParams.update({\n",
        "        # Font settings\n",
        "        \"font.family\": \"sans-serif\",\n",
        "        \"font.sans-serif\": [\"Arial\", \"Helvetica\", \"DejaVu Sans\"],\n",
        "        \"font.size\": 8,\n",
        "        \n",
        "        # Figure settings\n",
        "        \"figure.figsize\": (width, height),\n",
        "        \"figure.dpi\": 300,\n",
        "        \"figure.facecolor\": 'white',\n",
        "        \"figure.edgecolor\": 'white',\n",
        "        \"figure.constrained_layout.use\": True,\n",
        "        \n",
        "        # Axes settings\n",
        "        \"axes.labelsize\": 8,\n",
        "        \"axes.titlesize\": 9,\n",
        "        \"axes.titleweight\": 'bold',\n",
        "        \"axes.spines.top\": False,\n",
        "        \"axes.spines.right\": False,\n",
        "        \"axes.linewidth\": 0.5,\n",
        "        \"axes.facecolor\": 'white',\n",
        "        \"axes.edgecolor\": COLORS['slate'],\n",
        "        \"axes.labelcolor\": COLORS['charcoal'],\n",
        "        \"axes.prop_cycle\": plt.cycler(color=[\n",
        "            COLORS['gold'], COLORS['teal'], COLORS['blue'], \n",
        "            COLORS['orange'], COLORS['cyan'], COLORS['amber']\n",
        "        ]),\n",
        "        \n",
        "        # Tick settings\n",
        "        \"xtick.labelsize\": 8,\n",
        "        \"ytick.labelsize\": 8,\n",
        "        \"xtick.top\": False,\n",
        "        \"ytick.right\": False,\n",
        "        \"xtick.direction\": 'out',\n",
        "        \"ytick.direction\": 'out',\n",
        "        \"xtick.major.width\": 0.5,\n",
        "        \"ytick.major.width\": 0.5,\n",
        "        \"xtick.color\": COLORS['slate'],\n",
        "        \"ytick.color\": COLORS['slate'],\n",
        "        \n",
        "        # Legend settings\n",
        "        \"legend.fontsize\": 7,\n",
        "        \"legend.frameon\": False,\n",
        "        \"legend.loc\": 'upper left',\n",
        "        \n",
        "        # Grid settings\n",
        "        \"axes.grid\": True,\n",
        "        \"grid.alpha\": 0.3,\n",
        "        \"grid.linewidth\": 0.3,\n",
        "        \"grid.color\": COLORS['gray'],\n",
        "        \n",
        "        # Line settings\n",
        "        \"lines.linewidth\": 1.5,\n",
        "        \"lines.markersize\": 5,\n",
        "        \n",
        "        # Scatter settings\n",
        "        \"scatter.edgecolors\": 'none',\n",
        "        \n",
        "        # Save settings\n",
        "        \"savefig.dpi\": 300,\n",
        "        \"savefig.format\": 'pdf',\n",
        "        \"savefig.bbox\": 'tight',\n",
        "        \"savefig.pad_inches\": 0.05,\n",
        "        \"savefig.facecolor\": 'white',\n",
        "        \"savefig.edgecolor\": 'white',\n",
        "        \n",
        "        # PDF settings\n",
        "        \"pdf.fonttype\": 42,\n",
        "    })\n",
        "    \n",
        "    return {\n",
        "        'width': width,\n",
        "        'width2': width2, \n",
        "        'height': height,\n",
        "        'height2': height2,\n",
        "        'aspect_ratio': aspect_ratio,\n",
        "        'colors': COLORS,\n",
        "    }\n",
        "\n",
        "\n",
        "def save_figure(fig, name: str, output_dir: Path, dpi: int = 300,\n",
        "                formats: List[str] = ['png', 'pdf']):\n",
        "    \"\"\"Save figure in multiple formats with high quality settings.\"\"\"\n",
        "    for fmt in formats:\n",
        "        path = output_dir / f'{name}.{fmt}'\n",
        "        fig.savefig(path, dpi=dpi if fmt == 'png' else None,\n",
        "                    bbox_inches='tight', facecolor='white', edgecolor='none')\n",
        "    print(f\"Saved: {name} ({', '.join(formats)})\")\n",
        "\n",
        "\n",
        "# Initialize styling\n",
        "plot_dims = setup_plot_style()\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = Path('data')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "OUTPUT_DIR = Path('output')\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"Setup complete.\")\n",
        "print(f\"  Data dir: {DATA_DIR.absolute()}\")\n",
        "print(f\"  Output dir: {OUTPUT_DIR.absolute()}\")\n",
        "print(f\"  Figure size: {plot_dims['width']:.2f} x {plot_dims['height']:.2f} inches\")\n",
        "print(f\"  Color palette: {len(COLORS)} colors defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Config: ForecastConfig(start_year=2012, end_year=2025, forecast_horizon=3, forecast_origin=2026.0, n_bootstrap=2000, random_seed=42, confidence_levels=(0.5, 0.8, 0.9, 0.95), backtest_start=2018, backtest_end=2024, backtest_horizons=(1, 2, 3), models_to_compare=('naive', 'linear', 'exponential', 'piecewise', 'quadratic'))\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class ForecastConfig:\n",
        "    \"\"\"Configuration for publication-grade forecasting.\"\"\"\n",
        "    # Data range\n",
        "    start_year: int = 2012\n",
        "    end_year: int = 2025\n",
        "    forecast_horizon: int = 3  # years\n",
        "    forecast_origin: float = 2026.0\n",
        "    \n",
        "    # Bootstrap\n",
        "    n_bootstrap: int = 2000\n",
        "    random_seed: int = 42\n",
        "    \n",
        "    # Confidence levels\n",
        "    confidence_levels: Tuple[float, ...] = (0.50, 0.80, 0.90, 0.95)\n",
        "    \n",
        "    # Backtesting (rolling-origin)\n",
        "    backtest_start: int = 2018\n",
        "    backtest_end: int = 2024\n",
        "    backtest_horizons: Tuple[int, ...] = (1, 2, 3)  # years ahead\n",
        "    \n",
        "    # Model comparison\n",
        "    models_to_compare: Tuple[str, ...] = (\n",
        "        'naive',           # Last observed value\n",
        "        'linear',          # OLS linear trend\n",
        "        'exponential',     # Log-linear (exponential) trend\n",
        "        'piecewise',       # Piecewise linear with changepoint\n",
        "        'quadratic',       # Polynomial degree 2\n",
        "    )\n",
        "\n",
        "config = ForecastConfig()\n",
        "np.random.seed(config.random_seed)\n",
        "print(f\"Config: {config}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Threshold Taxonomy (Defensibility-Compliant)\n",
        "\n",
        "**Critical distinction:**\n",
        "- **Policy-defined thresholds** (solid lines): Evidence-backed regulatory triggers\n",
        "- **Compute milestones** (dotted lines): Training budget forecasts anchored to US EO threshold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Policy-Defined Thresholds (solid lines):\n",
            "  EU AI Act Systemic Risk: 10^25 FLOP\n",
            "    Source: EU AI Act Article 51(2)\n",
            "    Function: Triggers notification + risk management obligations\n",
            "  US EO 14110 Reporting: 10^26 FLOP\n",
            "    Source: Executive Order 14110 § 4.2(a)(i)\n",
            "    Function: Triggers reporting + red-teaming disclosure\n",
            "\n",
            "Compute Milestones (dotted lines - anchored to US EO threshold):\n",
            "  10× US EO (10²⁷): 10^27.0 FLOP\n",
            "    Justification: 10× beyond US EO reporting threshold (~2026 frontier)\n",
            "  100× US EO (10²⁸): 10^28.0 FLOP\n",
            "    Justification: 100× beyond US EO reporting threshold (~2027 frontier)\n",
            "  ~3000× US EO (10²⁹·⁵): 10^29.5 FLOP\n",
            "    Justification: ~3000× beyond US EO threshold (3-year horizon upper bound)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# THRESHOLD TAXONOMY (Defensibility-Compliant)\n",
        "# =============================================================================\n",
        "\n",
        "POLICY_DEFINED_THRESHOLDS = {\n",
        "    'EU AI Act Systemic Risk': {\n",
        "        'value': 25.0,\n",
        "        'source': 'EU AI Act Article 51(2)',\n",
        "        'function': 'Triggers notification + risk management obligations',\n",
        "        'line_style': '-',  # Solid line\n",
        "        'color': COLORS['green'],  # Already crossed\n",
        "    },\n",
        "    'US EO 14110 Reporting': {\n",
        "        'value': 26.0,\n",
        "        'source': 'Executive Order 14110 § 4.2(a)(i)',\n",
        "        'function': 'Triggers reporting + red-teaming disclosure',\n",
        "        'line_style': '-',  # Solid line\n",
        "        'color': COLORS['green'],  # Already crossed\n",
        "    },\n",
        "}\n",
        "\n",
        "SCENARIO_MARKERS = {\n",
        "    '10× US EO (10²⁷)': {\n",
        "        'value': 27.0,\n",
        "        'justification': '10× beyond US EO reporting threshold (~2026 frontier)',\n",
        "        'line_style': ':',  # Dotted line\n",
        "        'color': COLORS['gold'],\n",
        "        'disclaimer': 'Compute milestone, not capability claim',\n",
        "    },\n",
        "    '100× US EO (10²⁸)': {\n",
        "        'value': 28.0,\n",
        "        'justification': '100× beyond US EO reporting threshold (~2027 frontier)',\n",
        "        'line_style': ':',  # Dotted line\n",
        "        'color': COLORS['amber'],\n",
        "        'disclaimer': 'Compute milestone, not capability claim',\n",
        "    },\n",
        "    '~3000× US EO (10²⁹·⁵)': {\n",
        "        'value': 29.5,\n",
        "        'justification': '~3000× beyond US EO threshold (3-year horizon upper bound)',\n",
        "        'line_style': ':',  # Dotted line\n",
        "        'color': COLORS['orange'],\n",
        "        'disclaimer': 'Compute milestone, not capability claim',\n",
        "    },\n",
        "}\n",
        "\n",
        "# Combined for backward compatibility with existing code\n",
        "ALL_THRESHOLDS = {\n",
        "    **{name: info['value'] for name, info in POLICY_DEFINED_THRESHOLDS.items()},\n",
        "    **{name: info['value'] for name, info in SCENARIO_MARKERS.items()},\n",
        "}\n",
        "\n",
        "print(\"Policy-Defined Thresholds (solid lines):\")\n",
        "for name, info in POLICY_DEFINED_THRESHOLDS.items():\n",
        "    print(f\"  {name}: 10^{info['value']:.0f} FLOP\")\n",
        "    print(f\"    Source: {info['source']}\")\n",
        "    print(f\"    Function: {info['function']}\")\n",
        "    \n",
        "print(\"\\nCompute Milestones (dotted lines - anchored to US EO threshold):\")\n",
        "for name, info in SCENARIO_MARKERS.items():\n",
        "    print(f\"  {name}: 10^{info['value']:.1f} FLOP\")\n",
        "    print(f\"    Justification: {info['justification']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 444 models from Epoch AI data\n",
            "Year range: 2012 - 2025\n",
            "Compute range: 10^13.9 - 10^26.7 FLOP\n",
            "\n",
            "Frontier points: 14\n",
            "    year   log_flop\n",
            "9   2021  24.311118\n",
            "10  2022  24.437988\n",
            "11  2023  25.698970\n",
            "12  2024  25.579784\n",
            "13  2025  26.698970\n"
          ]
        }
      ],
      "source": [
        "# Load Epoch AI data\n",
        "df_raw = pd.read_csv(DATA_DIR / \"epoch_notable_models.csv\")\n",
        "\n",
        "# Parse dates and compute decimal year\n",
        "df_raw['date'] = pd.to_datetime(df_raw['Publication date'], errors='coerce')\n",
        "df_raw['year_decimal'] = df_raw['date'].dt.year + df_raw['date'].dt.dayofyear / 365.25\n",
        "df_raw['year'] = df_raw['date'].dt.year  # Integer year for grouping\n",
        "\n",
        "# Get training compute (log10)\n",
        "df_raw['log_flop'] = np.log10(df_raw['Training compute (FLOP)'].replace(0, np.nan))\n",
        "\n",
        "# Filter to valid data\n",
        "df = df_raw[['year', 'year_decimal', 'log_flop', 'Model']].dropna()\n",
        "df = df[(df['year'] >= config.start_year) & (df['year'] <= config.end_year)]\n",
        "\n",
        "print(f\"Loaded {len(df)} models from Epoch AI data\")\n",
        "print(f\"Year range: {df['year'].min()} - {df['year'].max()}\")\n",
        "print(f\"Compute range: 10^{df['log_flop'].min():.1f} - 10^{df['log_flop'].max():.1f} FLOP\")\n",
        "\n",
        "# Create frontier dataset (max compute per year)\n",
        "frontier_df = df.groupby('year').agg({\n",
        "    'log_flop': 'max',\n",
        "}).reset_index()\n",
        "\n",
        "frontier_df = frontier_df.sort_values('year')\n",
        "print(f\"\\nFrontier points: {len(frontier_df)}\")\n",
        "print(frontier_df.tail())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Definitions\n",
        "\n",
        "We define multiple forecasting models to compare:\n",
        "1. **Naive**: Persistence (last observed value)\n",
        "2. **Linear**: OLS linear trend\n",
        "3. **Exponential**: Log-linear trend\n",
        "4. **Piecewise**: Linear with optimal changepoint\n",
        "5. **Quadratic**: Polynomial degree 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model definitions loaded:\n",
            "  - naive\n",
            "  - linear\n",
            "  - exponential\n",
            "  - quadratic\n",
            "  - piecewise\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class ModelResult:\n",
        "    \"\"\"Container for model fit results.\"\"\"\n",
        "    name: str\n",
        "    fitted_values: np.ndarray\n",
        "    residuals: np.ndarray\n",
        "    predict_func: Callable[[np.ndarray], np.ndarray]\n",
        "    params: Dict[str, float]\n",
        "    aic: float\n",
        "    bic: float\n",
        "    r_squared: float\n",
        "    residual_std: float\n",
        "\n",
        "\n",
        "def fit_naive(X: np.ndarray, y: np.ndarray) -> ModelResult:\n",
        "    \"\"\"Naive persistence model - predicts last observed value.\"\"\"\n",
        "    fitted = np.full_like(y, y[-1])\n",
        "    residuals = y - fitted\n",
        "    n, k = len(y), 1\n",
        "    rss = np.sum(residuals**2)\n",
        "    tss = np.sum((y - np.mean(y))**2)\n",
        "    \n",
        "    return ModelResult(\n",
        "        name='naive',\n",
        "        fitted_values=fitted,\n",
        "        residuals=residuals,\n",
        "        predict_func=lambda x: np.full(len(x), y[-1]),\n",
        "        params={'last_value': y[-1]},\n",
        "        aic=n * np.log(rss/n) + 2*k,\n",
        "        bic=n * np.log(rss/n) + k*np.log(n),\n",
        "        r_squared=0.0,  # Naive baseline has R²=0 by definition\n",
        "        residual_std=np.std(residuals, ddof=1),\n",
        "    )\n",
        "\n",
        "\n",
        "def fit_linear(X: np.ndarray, y: np.ndarray) -> ModelResult:\n",
        "    \"\"\"Simple linear trend: y = a + b*x\"\"\"\n",
        "    model = LinearRegression()\n",
        "    model.fit(X.reshape(-1, 1), y)\n",
        "    fitted = model.predict(X.reshape(-1, 1))\n",
        "    residuals = y - fitted\n",
        "    n, k = len(y), 2\n",
        "    rss = np.sum(residuals**2)\n",
        "    tss = np.sum((y - np.mean(y))**2)\n",
        "    \n",
        "    return ModelResult(\n",
        "        name='linear',\n",
        "        fitted_values=fitted,\n",
        "        residuals=residuals,\n",
        "        predict_func=lambda x: model.predict(x.reshape(-1, 1)),\n",
        "        params={'intercept': model.intercept_, 'slope': model.coef_[0]},\n",
        "        aic=n * np.log(rss/n) + 2*k,\n",
        "        bic=n * np.log(rss/n) + k*np.log(n),\n",
        "        r_squared=1 - rss/tss if tss > 0 else 0,\n",
        "        residual_std=np.std(residuals, ddof=k),\n",
        "    )\n",
        "\n",
        "\n",
        "def fit_quadratic(X: np.ndarray, y: np.ndarray) -> ModelResult:\n",
        "    \"\"\"Quadratic trend: y = a + b*x + c*x²\"\"\"\n",
        "    poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "    X_poly = poly.fit_transform(X.reshape(-1, 1))\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_poly, y)\n",
        "    fitted = model.predict(X_poly)\n",
        "    residuals = y - fitted\n",
        "    n, k = len(y), 3\n",
        "    rss = np.sum(residuals**2)\n",
        "    tss = np.sum((y - np.mean(y))**2)\n",
        "    \n",
        "    def predict_func(x):\n",
        "        x_poly = poly.transform(x.reshape(-1, 1))\n",
        "        return model.predict(x_poly)\n",
        "    \n",
        "    return ModelResult(\n",
        "        name='quadratic',\n",
        "        fitted_values=fitted,\n",
        "        residuals=residuals,\n",
        "        predict_func=predict_func,\n",
        "        params={'intercept': model.intercept_, 'coef_1': model.coef_[0], 'coef_2': model.coef_[1]},\n",
        "        aic=n * np.log(rss/n) + 2*k,\n",
        "        bic=n * np.log(rss/n) + k*np.log(n),\n",
        "        r_squared=1 - rss/tss if tss > 0 else 0,\n",
        "        residual_std=np.std(residuals, ddof=k),\n",
        "    )\n",
        "\n",
        "\n",
        "def fit_piecewise(X: np.ndarray, y: np.ndarray, min_segment: int = 3) -> ModelResult:\n",
        "    \"\"\"Piecewise linear with optimal changepoint detection.\"\"\"\n",
        "    n = len(X)\n",
        "    if n < 2 * min_segment:\n",
        "        return fit_linear(X, y)  # Fall back to linear\n",
        "    \n",
        "    best_rss = np.inf\n",
        "    best_cp = None\n",
        "    best_models = None\n",
        "    \n",
        "    for cp_idx in range(min_segment, n - min_segment):\n",
        "        # Fit two linear models\n",
        "        X1, y1 = X[:cp_idx], y[:cp_idx]\n",
        "        X2, y2 = X[cp_idx:], y[cp_idx:]\n",
        "        \n",
        "        model1 = LinearRegression().fit(X1.reshape(-1, 1), y1)\n",
        "        model2 = LinearRegression().fit(X2.reshape(-1, 1), y2)\n",
        "        \n",
        "        rss1 = np.sum((y1 - model1.predict(X1.reshape(-1, 1)))**2)\n",
        "        rss2 = np.sum((y2 - model2.predict(X2.reshape(-1, 1)))**2)\n",
        "        total_rss = rss1 + rss2\n",
        "        \n",
        "        if total_rss < best_rss:\n",
        "            best_rss = total_rss\n",
        "            best_cp = X[cp_idx]\n",
        "            best_models = (model1, model2)\n",
        "    \n",
        "    if best_models is None:\n",
        "        return fit_linear(X, y)\n",
        "    \n",
        "    model1, model2 = best_models\n",
        "    \n",
        "    def predict_func(x):\n",
        "        result = np.zeros(len(x))\n",
        "        mask1 = x < best_cp\n",
        "        mask2 = ~mask1\n",
        "        if mask1.any():\n",
        "            result[mask1] = model1.predict(x[mask1].reshape(-1, 1))\n",
        "        if mask2.any():\n",
        "            result[mask2] = model2.predict(x[mask2].reshape(-1, 1))\n",
        "        return result\n",
        "    \n",
        "    fitted = predict_func(X)\n",
        "    residuals = y - fitted\n",
        "    tss = np.sum((y - np.mean(y))**2)\n",
        "    k = 4  # 2 slopes + 2 intercepts\n",
        "    \n",
        "    return ModelResult(\n",
        "        name='piecewise',\n",
        "        fitted_values=fitted,\n",
        "        residuals=residuals,\n",
        "        predict_func=predict_func,\n",
        "        params={\n",
        "            'changepoint': best_cp,\n",
        "            'slope_before': model1.coef_[0],\n",
        "            'slope_after': model2.coef_[0],\n",
        "        },\n",
        "        aic=n * np.log(best_rss/n) + 2*k,\n",
        "        bic=n * np.log(best_rss/n) + k*np.log(n),\n",
        "        r_squared=1 - best_rss/tss if tss > 0 else 0,\n",
        "        residual_std=np.std(residuals, ddof=k),\n",
        "    )\n",
        "\n",
        "\n",
        "def fit_exponential(X: np.ndarray, y: np.ndarray) -> ModelResult:\n",
        "    \"\"\"\n",
        "    Exponential trend model.\n",
        "    \n",
        "    NOTE: For log-transformed data (like log₁₀FLOP), exponential growth appears \n",
        "    as a linear trend. This model is mathematically equivalent to fit_linear \n",
        "    for our data. We include it for methodological completeness and to demonstrate \n",
        "    awareness that exponential ≡ linear on log scale.\n",
        "    \n",
        "    For non-log data, this would fit: y = a * exp(b*x)\n",
        "    For log data, this reduces to: log(y) = log(a) + b*x (linear)\n",
        "    \"\"\"\n",
        "    result = fit_linear(X, y)\n",
        "    result.name = 'exponential'  # Rename for clarity\n",
        "    return result\n",
        "\n",
        "\n",
        "# Model fitter registry\n",
        "MODEL_FITTERS = {\n",
        "    'naive': fit_naive,\n",
        "    'linear': fit_linear,\n",
        "    'exponential': fit_exponential,\n",
        "    'quadratic': fit_quadratic,\n",
        "    'piecewise': fit_piecewise,\n",
        "}\n",
        "\n",
        "print(\"Model definitions loaded:\")\n",
        "for name in MODEL_FITTERS:\n",
        "    print(f\"  - {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Fit All Models & Compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Comparison:\n",
            "======================================================================\n",
            "Model                 R²        AIC        BIC   Residual σ\n",
            "----------------------------------------------------------------------\n",
            "naive              0.000      46.70      47.33        2.713\n",
            "linear             0.966     -16.57     -15.29        0.518\n",
            "exponential        0.966     -16.57     -15.29        0.518\n",
            "quadratic          0.969     -15.61     -13.69        0.521\n",
            "piecewise          0.985     -23.94     -21.38        0.378\n",
            "\n",
            "Best model by BIC (excluding exponential*): piecewise\n",
            "Selected model for forecasting: piecewise\n",
            "  Slope: 0.671 log₁₀FLOP/year\n",
            "\n",
            "* Note: Exponential model on log-scale data is mathematically identical to linear.\n",
            "  We include it for completeness but exclude from model selection.\n"
          ]
        }
      ],
      "source": [
        "X = frontier_df['year'].values.astype(float)\n",
        "y = frontier_df['log_flop'].values\n",
        "\n",
        "# Fit all models\n",
        "models = {}\n",
        "for name, fitter in MODEL_FITTERS.items():\n",
        "    models[name] = fitter(X, y)\n",
        "\n",
        "# Compare models\n",
        "print(\"Model Comparison:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Model':<15} {'R²':>8} {'AIC':>10} {'BIC':>10} {'Residual σ':>12}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "comparison_data = []\n",
        "for name, result in models.items():\n",
        "    print(f\"{name:<15} {result.r_squared:>8.3f} {result.aic:>10.2f} {result.bic:>10.2f} {result.residual_std:>12.3f}\")\n",
        "    comparison_data.append({\n",
        "        'model': name,\n",
        "        'r_squared': result.r_squared,\n",
        "        'aic': result.aic,\n",
        "        'bic': result.bic,\n",
        "        'residual_std': result.residual_std,\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Best model by BIC (excluding exponential which is redundant with linear for log data)\n",
        "# Note: exponential on log-scale data = linear, so we exclude it from selection\n",
        "selectable_models = comparison_df[comparison_df['model'] != 'exponential']\n",
        "best_by_bic = selectable_models.loc[selectable_models['bic'].idxmin(), 'model']\n",
        "print(f\"\\nBest model by BIC (excluding exponential*): {best_by_bic}\")\n",
        "\n",
        "# Use the BIC-selected model\n",
        "selected_model = best_by_bic\n",
        "print(f\"Selected model for forecasting: {selected_model}\")\n",
        "print(f\"  Slope: {models[selected_model].params.get('slope', models[selected_model].params.get('slope_after', 'N/A')):.3f} log₁₀FLOP/year\")\n",
        "if 'slope' in models[selected_model].params:\n",
        "    print(f\"  That's {10**models[selected_model].params.get('slope', 0):.2f}x compute growth per year\")\n",
        "\n",
        "print(\"\\n* Note: Exponential model on log-scale data is mathematically identical to linear.\")\n",
        "print(\"  We include it for completeness but exclude from model selection.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: model_comparison (png, pdf)\n"
          ]
        }
      ],
      "source": [
        "# Visualize model fits (with consistent palette)\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "# Data points\n",
        "ax.scatter(X, y, s=100, c=COLORS['gold'], edgecolors='white', linewidth=2, \n",
        "           zorder=10, label='Historical frontier')\n",
        "\n",
        "# Plot each model fit\n",
        "x_range = np.linspace(X.min() - 1, X.max() + 4, 100)\n",
        "model_colors = {\n",
        "    'naive': COLORS['gray'],\n",
        "    'linear': COLORS['blue'],\n",
        "    'exponential': COLORS['teal'],\n",
        "    'quadratic': COLORS['purple'],\n",
        "    'piecewise': COLORS['orange'],\n",
        "}\n",
        "\n",
        "for name, result in models.items():\n",
        "    color = model_colors.get(name, COLORS['gray'])\n",
        "    pred = result.predict_func(x_range)\n",
        "    linestyle = '-' if name == selected_model else '--'\n",
        "    linewidth = 2.5 if name == selected_model else 1.5\n",
        "    alpha = 1.0 if name == selected_model else 0.6\n",
        "    ax.plot(x_range, pred, color=color, linestyle=linestyle, \n",
        "            linewidth=linewidth, alpha=alpha, label=f'{name} (R²={result.r_squared:.2f})')\n",
        "\n",
        "ax.set_xlabel('Year', fontsize=11)\n",
        "ax.set_ylabel('log₁₀(Training FLOP)', fontsize=11)\n",
        "ax.set_title('Model Comparison: Frontier AI Compute Growth', fontsize=12)\n",
        "ax.legend(loc='upper left', fontsize=9)\n",
        "ax.set_xlim(2010, 2030)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure(fig, 'model_comparison', OUTPUT_DIR, dpi=300)\n",
        "# plt.show()  # Disabled for batch execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Rolling-Origin Cross-Validation\n",
        "\n",
        "Proper backtesting with multiple forecast horizons (1, 2, 3 years ahead)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running rolling-origin backtest for all models...\n",
            "  Backtesting naive...\n",
            "    18 forecasts, 90% coverage: 77.8%\n",
            "  Backtesting linear...\n",
            "    18 forecasts, 90% coverage: 100.0%\n",
            "  Backtesting exponential...\n",
            "    18 forecasts, 90% coverage: 100.0%\n",
            "  Backtesting quadratic...\n",
            "    18 forecasts, 90% coverage: 83.3%\n",
            "  Backtesting piecewise...\n",
            "    18 forecasts, 90% coverage: 88.9%\n",
            "\n",
            "Backtest complete.\n"
          ]
        }
      ],
      "source": [
        "@dataclass\n",
        "class BacktestResult:\n",
        "    \"\"\"Single backtest result.\"\"\"\n",
        "    origin_year: int\n",
        "    horizon: int\n",
        "    target_year: int\n",
        "    actual: float\n",
        "    predicted: float\n",
        "    error: float\n",
        "    # Bootstrap prediction intervals\n",
        "    pi_50_lo: float\n",
        "    pi_50_hi: float\n",
        "    pi_80_lo: float\n",
        "    pi_80_hi: float\n",
        "    pi_90_lo: float\n",
        "    pi_90_hi: float\n",
        "    pi_95_lo: float\n",
        "    pi_95_hi: float\n",
        "    covered: bool = field(default=False)  # Is actual within 90% PI?\n",
        "    \n",
        "    def __post_init__(self):\n",
        "        self.covered = self.pi_90_lo <= self.actual <= self.pi_90_hi\n",
        "\n",
        "\n",
        "def rolling_origin_backtest(\n",
        "    frontier_df: pd.DataFrame,\n",
        "    model_fitter: Callable,\n",
        "    config: ForecastConfig,\n",
        "    n_bootstrap: int = 1000,\n",
        ") -> List[BacktestResult]:\n",
        "    \"\"\"Rolling-origin cross-validation with bootstrap uncertainty.\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    # Get available years in data\n",
        "    available_years = set(frontier_df['year'].values)\n",
        "    \n",
        "    for origin_year in range(config.backtest_start, config.backtest_end + 1):\n",
        "        for horizon in config.backtest_horizons:\n",
        "            target_year = origin_year + horizon\n",
        "            \n",
        "            # Check if we have data for target year\n",
        "            if target_year not in available_years:\n",
        "                continue\n",
        "            \n",
        "            # Training data: up to origin_year (inclusive)\n",
        "            train_df = frontier_df[frontier_df['year'] <= origin_year]\n",
        "            if len(train_df) < 5:  # Need minimum data\n",
        "                continue\n",
        "            \n",
        "            X_train = train_df['year'].values.astype(float)\n",
        "            y_train = train_df['log_flop'].values\n",
        "            \n",
        "            # Actual value at target year\n",
        "            actual_row = frontier_df[frontier_df['year'] == target_year]\n",
        "            if len(actual_row) == 0:\n",
        "                continue\n",
        "            actual = actual_row['log_flop'].values[0]\n",
        "            \n",
        "            # Fit model and predict\n",
        "            model = model_fitter(X_train, y_train)\n",
        "            predicted = model.predict_func(np.array([float(target_year)]))[0]\n",
        "            \n",
        "            # Bootstrap for prediction intervals\n",
        "            boot_preds = []\n",
        "            for _ in range(n_bootstrap):\n",
        "                # Residual bootstrap\n",
        "                boot_residuals = np.random.choice(model.residuals, size=len(y_train), replace=True)\n",
        "                y_boot = model.fitted_values + boot_residuals\n",
        "                \n",
        "                boot_model = model_fitter(X_train, y_boot)\n",
        "                boot_pred = boot_model.predict_func(np.array([float(target_year)]))[0]\n",
        "                # Add forecast noise\n",
        "                boot_pred += np.random.normal(0, model.residual_std)\n",
        "                boot_preds.append(boot_pred)\n",
        "            \n",
        "            boot_preds = np.array(boot_preds)\n",
        "            \n",
        "            results.append(BacktestResult(\n",
        "                origin_year=origin_year,\n",
        "                horizon=horizon,\n",
        "                target_year=target_year,\n",
        "                actual=actual,\n",
        "                predicted=predicted,\n",
        "                error=actual - predicted,\n",
        "                pi_50_lo=np.percentile(boot_preds, 25),\n",
        "                pi_50_hi=np.percentile(boot_preds, 75),\n",
        "                pi_80_lo=np.percentile(boot_preds, 10),\n",
        "                pi_80_hi=np.percentile(boot_preds, 90),\n",
        "                pi_90_lo=np.percentile(boot_preds, 5),\n",
        "                pi_90_hi=np.percentile(boot_preds, 95),\n",
        "                pi_95_lo=np.percentile(boot_preds, 2.5),\n",
        "                pi_95_hi=np.percentile(boot_preds, 97.5),\n",
        "            ))\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Run backtest for all models\n",
        "print(\"Running rolling-origin backtest for all models...\")\n",
        "backtest_results = {}\n",
        "\n",
        "for name, fitter in MODEL_FITTERS.items():\n",
        "    print(f\"  Backtesting {name}...\")\n",
        "    backtest_results[name] = rolling_origin_backtest(\n",
        "        frontier_df, fitter, config, n_bootstrap=500\n",
        "    )\n",
        "    n_results = len(backtest_results[name])\n",
        "    coverage = np.mean([r.covered for r in backtest_results[name]]) if n_results > 0 else 0\n",
        "    print(f\"    {n_results} forecasts, 90% coverage: {coverage:.1%}\")\n",
        "\n",
        "print(\"\\nBacktest complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Probabilistic Scoring: CRPS, Coverage, and Calibration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model Performance Summary (with WIS):\n",
            "==========================================================================================\n",
            "Model             MAE     RMSE     CRPS      WIS    Cov@50%    Cov@90%\n",
            "------------------------------------------------------------------------------------------\n",
            "naive           1.212    1.360    0.915    2.273       0.0%      77.8%\n",
            "linear          0.252    0.345    0.242    0.091      72.2%     100.0%\n",
            "exponential     0.252    0.345    0.243    0.105      72.2%     100.0%\n",
            "quadratic       1.097    1.449    0.757    0.545      38.9%      83.3%\n",
            "piecewise       0.632    0.754    0.432    0.308      44.4%      88.9%\n",
            "------------------------------------------------------------------------------------------\n",
            "Note: WIS = Weighted Interval Score (proper scoring rule for interval forecasts).\n"
          ]
        }
      ],
      "source": [
        "def compute_crps_empirical(actual: float, samples: np.ndarray) -> float:\n",
        "    \"\"\"CRPS using empirical bootstrap samples (more accurate than Gaussian approx).\"\"\"\n",
        "    if HAS_PROPERSCORING:\n",
        "        return ps.crps_ensemble(actual, samples)\n",
        "    else:\n",
        "        # Fallback: empirical CRPS\n",
        "        n = len(samples)\n",
        "        term1 = np.mean(np.abs(samples - actual))\n",
        "        term2 = np.mean(np.abs(samples[:, None] - samples[None, :])) / 2\n",
        "        return term1 - term2\n",
        "\n",
        "\n",
        "def compute_wis(actual: float, quantiles: Dict[float, float], alpha_levels: List[float] = None) -> float:\n",
        "    \"\"\"\n",
        "    Weighted Interval Score (WIS) - proper scoring rule for interval forecasts.\n",
        "    \n",
        "    WIS = (1/K) * sum_k [ alpha_k/2 * width_k + indicator_below * (lower - actual) \n",
        "                                                + indicator_above * (actual - upper) ]\n",
        "    \n",
        "    Args:\n",
        "        actual: observed value\n",
        "        quantiles: dict mapping probability levels to quantile values\n",
        "                   e.g., {0.05: lower_90, 0.95: upper_90, 0.25: lower_50, 0.75: upper_50}\n",
        "        alpha_levels: coverage levels to use (default: [0.5, 0.8, 0.9])\n",
        "    \"\"\"\n",
        "    if alpha_levels is None:\n",
        "        alpha_levels = [0.5, 0.8, 0.9]\n",
        "    \n",
        "    wis = 0\n",
        "    for alpha in alpha_levels:\n",
        "        lower_p = (1 - alpha) / 2\n",
        "        upper_p = 1 - lower_p\n",
        "        \n",
        "        if lower_p in quantiles and upper_p in quantiles:\n",
        "            lower = quantiles[lower_p]\n",
        "            upper = quantiles[upper_p]\n",
        "            width = upper - lower\n",
        "            \n",
        "            # Interval score components\n",
        "            underprediction = 2/alpha * (lower - actual) * (actual < lower)\n",
        "            overprediction = 2/alpha * (actual - upper) * (actual > upper)\n",
        "            \n",
        "            wis += (alpha/2) * width + underprediction + overprediction\n",
        "    \n",
        "    return wis / len(alpha_levels)\n",
        "\n",
        "\n",
        "def compute_crps_gaussian(actual: float, mean: float, std: float) -> float:\n",
        "    \"\"\"CRPS for Gaussian predictive distribution (less accurate, for reference).\"\"\"\n",
        "    if HAS_PROPERSCORING:\n",
        "        return ps.crps_gaussian(actual, mu=mean, sig=std)\n",
        "    else:\n",
        "        z = (actual - mean) / std\n",
        "        return std * (z * (2 * stats.norm.cdf(z) - 1) + \n",
        "                     2 * stats.norm.pdf(z) - 1 / np.sqrt(np.pi))\n",
        "\n",
        "\n",
        "def compute_coverage(results: List[BacktestResult], level: float) -> float:\n",
        "    \"\"\"Compute empirical coverage at given confidence level.\"\"\"\n",
        "    if not results:\n",
        "        return np.nan\n",
        "    \n",
        "    if level == 0.90:\n",
        "        covered = [r.pi_90_lo <= r.actual <= r.pi_90_hi for r in results]\n",
        "    elif level == 0.50:\n",
        "        covered = [r.pi_50_lo <= r.actual <= r.pi_50_hi for r in results]\n",
        "    elif level == 0.80:\n",
        "        covered = [r.pi_80_lo <= r.actual <= r.pi_80_hi for r in results]\n",
        "    elif level == 0.95:\n",
        "        covered = [r.pi_95_lo <= r.actual <= r.pi_95_hi for r in results]\n",
        "    else:\n",
        "        return np.nan\n",
        "    \n",
        "    return np.mean(covered)\n",
        "\n",
        "\n",
        "# Compute metrics for each model (with WIS)\n",
        "print(\"Model Performance Summary (with WIS):\")\n",
        "print(\"=\" * 90)\n",
        "print(f\"{'Model':<12} {'MAE':>8} {'RMSE':>8} {'CRPS':>8} {'WIS':>8} {'Cov@50%':>10} {'Cov@90%':>10}\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "for name, results in backtest_results.items():\n",
        "    if not results:\n",
        "        continue\n",
        "    \n",
        "    mae = np.mean([abs(r.error) for r in results])\n",
        "    rmse = np.sqrt(np.mean([r.error**2 for r in results]))\n",
        "    \n",
        "    # CRPS using Gaussian approximation (for comparison)\n",
        "    crps_values = []\n",
        "    wis_values = []\n",
        "    for r in results:\n",
        "        std_approx = (r.pi_95_hi - r.pi_95_lo) / (2 * 1.96)\n",
        "        crps = compute_crps_gaussian(r.actual, r.predicted, std_approx)\n",
        "        crps_values.append(crps)\n",
        "        \n",
        "        # WIS with quantiles\n",
        "        quantiles = {\n",
        "            0.025: r.pi_95_lo, 0.975: r.pi_95_hi,\n",
        "            0.05: r.pi_90_lo, 0.95: r.pi_90_hi,\n",
        "            0.10: r.pi_80_lo, 0.90: r.pi_80_hi,\n",
        "            0.25: r.pi_50_lo, 0.75: r.pi_50_hi,\n",
        "        }\n",
        "        wis = compute_wis(r.actual, quantiles, alpha_levels=[0.5, 0.8, 0.9, 0.95])\n",
        "        wis_values.append(wis)\n",
        "    \n",
        "    crps_mean = np.mean(crps_values)\n",
        "    wis_mean = np.mean(wis_values)\n",
        "    \n",
        "    cov_50 = compute_coverage(results, 0.50)\n",
        "    cov_90 = compute_coverage(results, 0.90)\n",
        "    \n",
        "    print(f\"{name:<12} {mae:>8.3f} {rmse:>8.3f} {crps_mean:>8.3f} {wis_mean:>8.3f} {cov_50:>10.1%} {cov_90:>10.1%}\")\n",
        "\n",
        "print(\"-\" * 90)\n",
        "print(\"Note: WIS = Weighted Interval Score (proper scoring rule for interval forecasts).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: calibration_analysis (png, pdf)\n"
          ]
        }
      ],
      "source": [
        "# Calibration plot (using consistent palette)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Left: Coverage vs nominal for selected model\n",
        "ax = axes[0]\n",
        "results = backtest_results[selected_model]\n",
        "nominal_levels = [0.50, 0.80, 0.90, 0.95]\n",
        "empirical_coverages = [compute_coverage(results, level) for level in nominal_levels]\n",
        "\n",
        "ax.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Perfect calibration')\n",
        "ax.scatter(nominal_levels, empirical_coverages, s=100, c=COLORS['blue'], \n",
        "           edgecolors='white', linewidth=2, zorder=5)\n",
        "ax.plot(nominal_levels, empirical_coverages, color=COLORS['blue'], alpha=0.5)\n",
        "\n",
        "for nom, emp in zip(nominal_levels, empirical_coverages):\n",
        "    ax.annotate(f'{emp:.0%}', (nom, emp), textcoords=\"offset points\", \n",
        "                xytext=(5, 10), fontsize=9)\n",
        "\n",
        "ax.set_xlabel('Nominal Coverage', fontsize=11)\n",
        "ax.set_ylabel('Empirical Coverage', fontsize=11)\n",
        "ax.set_title(f'Calibration: {selected_model.title()} Model', fontsize=12)\n",
        "ax.set_xlim(0.4, 1.0)\n",
        "ax.set_ylim(0.4, 1.0)\n",
        "ax.legend(loc='lower right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Right: PIT histogram\n",
        "ax = axes[1]\n",
        "pit_values = []\n",
        "for r in results:\n",
        "    # Approximate PIT using Gaussian assumption\n",
        "    std_approx = (r.pi_95_hi - r.pi_95_lo) / (2 * 1.96)\n",
        "    pit = stats.norm.cdf(r.actual, loc=r.predicted, scale=std_approx)\n",
        "    pit_values.append(pit)\n",
        "\n",
        "ax.hist(pit_values, bins=10, density=True, alpha=0.7, color=COLORS['teal'], \n",
        "        edgecolor=COLORS['charcoal'], linewidth=0.5)\n",
        "ax.axhline(1.0, color=COLORS['red'], linestyle='--', linewidth=1.5, \n",
        "           label='Uniform (well-calibrated)')\n",
        "ax.set_xlabel('PIT Value', fontsize=11)\n",
        "ax.set_ylabel('Density', fontsize=11)\n",
        "ax.set_title('PIT Histogram', fontsize=12)\n",
        "ax.legend(loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure(fig, 'calibration_analysis', OUTPUT_DIR, dpi=300)\n",
        "# plt.show()  # Disabled for batch execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Performance by Forecast Horizon:\n",
            "============================================================\n",
            "\n",
            "1-Year Ahead Forecasts:\n",
            "----------------------------------------\n",
            "  naive       : MAE=0.714, Coverage=100.0%\n",
            "  linear      : MAE=0.243, Coverage=100.0%\n",
            "  exponential : MAE=0.243, Coverage=100.0%\n",
            "  quadratic   : MAE=0.676, Coverage=85.7%\n",
            "  piecewise   : MAE=0.460, Coverage=85.7%\n",
            "\n",
            "2-Year Ahead Forecasts:\n",
            "----------------------------------------\n",
            "  naive       : MAE=1.217, Coverage=83.3%\n",
            "  linear      : MAE=0.232, Coverage=100.0%\n",
            "  exponential : MAE=0.232, Coverage=100.0%\n",
            "  quadratic   : MAE=1.021, Coverage=83.3%\n",
            "  piecewise   : MAE=0.688, Coverage=100.0%\n",
            "\n",
            "3-Year Ahead Forecasts:\n",
            "----------------------------------------\n",
            "  naive       : MAE=1.901, Coverage=40.0%\n",
            "  linear      : MAE=0.286, Coverage=100.0%\n",
            "  exponential : MAE=0.286, Coverage=100.0%\n",
            "  quadratic   : MAE=1.779, Coverage=80.0%\n",
            "  piecewise   : MAE=0.807, Coverage=80.0%\n"
          ]
        }
      ],
      "source": [
        "# Performance breakdown by forecast horizon\n",
        "print(\"\\nPerformance by Forecast Horizon:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for horizon in config.backtest_horizons:\n",
        "    print(f\"\\n{horizon}-Year Ahead Forecasts:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    for name, results in backtest_results.items():\n",
        "        horizon_results = [r for r in results if r.horizon == horizon]\n",
        "        if not horizon_results:\n",
        "            continue\n",
        "        \n",
        "        mae = np.mean([abs(r.error) for r in horizon_results])\n",
        "        cov = np.mean([r.covered for r in horizon_results])\n",
        "        print(f\"  {name:<12}: MAE={mae:.3f}, Coverage={cov:.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Empirical Recalibration\n",
        "\n",
        "Fix under-coverage by widening prediction intervals based on backtest errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calibration Factors (to achieve 90% coverage):\n",
            "==================================================\n",
            "  naive: factor=1.16 (current 90% coverage: 78%)\n",
            "  linear: factor=1.00 (current 90% coverage: 100%)\n",
            "  exponential: factor=1.00 (current 90% coverage: 100%)\n",
            "  quadratic: factor=1.21 (current 90% coverage: 83%)\n",
            "  piecewise: factor=1.15 (current 90% coverage: 89%)\n",
            "\n",
            "→ Use these factors to widen prediction intervals in production forecasts.\n"
          ]
        }
      ],
      "source": [
        "def compute_calibration_factor(results: List[BacktestResult], target_coverage: float = 0.90) -> float:\n",
        "    \"\"\"\n",
        "    Compute multiplicative factor to achieve target coverage.\n",
        "    Based on empirical backtest errors.\n",
        "    \"\"\"\n",
        "    # Current 90% interval widths\n",
        "    widths = [(r.pi_90_hi - r.pi_90_lo) for r in results]\n",
        "    errors = [abs(r.error) for r in results]\n",
        "    \n",
        "    # Current coverage\n",
        "    current_cov = compute_coverage(results, 0.90)\n",
        "    \n",
        "    if current_cov >= target_coverage:\n",
        "        return 1.0  # Already well-calibrated or over-covered\n",
        "    \n",
        "    # Find factor that achieves target coverage\n",
        "    # Binary search for optimal factor\n",
        "    def coverage_at_factor(factor):\n",
        "        covered = 0\n",
        "        for r in results:\n",
        "            width = r.pi_90_hi - r.pi_90_lo\n",
        "            center = (r.pi_90_hi + r.pi_90_lo) / 2\n",
        "            new_lo = center - factor * width / 2\n",
        "            new_hi = center + factor * width / 2\n",
        "            if new_lo <= r.actual <= new_hi:\n",
        "                covered += 1\n",
        "        return covered / len(results)\n",
        "    \n",
        "    # Search for factor\n",
        "    lo, hi = 1.0, 3.0\n",
        "    for _ in range(20):  # Binary search iterations\n",
        "        mid = (lo + hi) / 2\n",
        "        cov = coverage_at_factor(mid)\n",
        "        if cov < target_coverage:\n",
        "            lo = mid\n",
        "        else:\n",
        "            hi = mid\n",
        "    \n",
        "    return hi\n",
        "\n",
        "\n",
        "# Compute calibration factors for each model\n",
        "print(\"Calibration Factors (to achieve 90% coverage):\")\n",
        "print(\"=\" * 50)\n",
        "calibration_factors = {}\n",
        "\n",
        "for name, results in backtest_results.items():\n",
        "    if not results:\n",
        "        continue\n",
        "    factor = compute_calibration_factor(results, target_coverage=0.90)\n",
        "    current_cov = compute_coverage(results, 0.90)\n",
        "    calibration_factors[name] = factor\n",
        "    print(f\"  {name}: factor={factor:.2f} (current 90% coverage: {current_cov:.0%})\")\n",
        "\n",
        "print(\"\\n→ Use these factors to widen prediction intervals in production forecasts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Calibrated Forecasts\n",
        "\n",
        "Generate final forecasts with calibrated prediction intervals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Calibrated Forecast (piecewise model):\n",
            "  Calibration factor: 1.15\n",
            "  R²: 0.985\n",
            "  Slope (recent): 0.671 log₁₀FLOP/year\n",
            "  Slope (early): 1.025 log₁₀FLOP/year\n",
            "  Changepoint: 2017.0\n",
            "  Growth rate (recent): 4.69x/year\n",
            "\n",
            "Forecast Summary:\n",
            "  2026: 27.37 [26.53, 28.22]\n",
            "  2027: 28.06 [27.16, 28.97]\n",
            "  2028: 28.71 [27.77, 29.65]\n",
            "  2029: 29.40 [28.43, 30.37]\n"
          ]
        }
      ],
      "source": [
        "def generate_calibrated_forecast(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    model_fitter: Callable,\n",
        "    future_years: np.ndarray,\n",
        "    calibration_factor: float = 1.0,\n",
        "    n_bootstrap: int = 2000,\n",
        ") -> Dict:\n",
        "    \"\"\"Generate forecast with calibrated prediction intervals.\"\"\"\n",
        "    # Fit model\n",
        "    model = model_fitter(X, y)\n",
        "    \n",
        "    # Bootstrap for uncertainty\n",
        "    boot_forecasts = np.zeros((n_bootstrap, len(future_years)))\n",
        "    \n",
        "    for b in range(n_bootstrap):\n",
        "        # Residual bootstrap\n",
        "        boot_residuals = np.random.choice(model.residuals, size=len(y), replace=True)\n",
        "        y_boot = model.fitted_values + boot_residuals\n",
        "        \n",
        "        boot_model = model_fitter(X, y_boot)\n",
        "        boot_pred = boot_model.predict_func(future_years)\n",
        "        boot_pred += np.random.normal(0, model.residual_std, len(future_years))\n",
        "        boot_forecasts[b] = boot_pred\n",
        "    \n",
        "    # Point forecast\n",
        "    forecast_median = np.median(boot_forecasts, axis=0)\n",
        "    \n",
        "    # Raw percentiles\n",
        "    raw_lo_90 = np.percentile(boot_forecasts, 5, axis=0)\n",
        "    raw_hi_90 = np.percentile(boot_forecasts, 95, axis=0)\n",
        "    \n",
        "    # Apply calibration factor\n",
        "    raw_width = raw_hi_90 - raw_lo_90\n",
        "    calibrated_width = raw_width * calibration_factor\n",
        "    \n",
        "    # Calibrated intervals (centered on median)\n",
        "    result = {\n",
        "        'years': future_years,\n",
        "        'median': forecast_median,\n",
        "        'model_params': model.params,\n",
        "        'r_squared': model.r_squared,\n",
        "        'calibration_factor': calibration_factor,\n",
        "    }\n",
        "    \n",
        "    # Add all confidence levels\n",
        "    for level in [0.50, 0.80, 0.90, 0.95]:\n",
        "        alpha = 1 - level\n",
        "        raw_lo = np.percentile(boot_forecasts, 100*alpha/2, axis=0)\n",
        "        raw_hi = np.percentile(boot_forecasts, 100*(1-alpha/2), axis=0)\n",
        "        raw_w = raw_hi - raw_lo\n",
        "        cal_w = raw_w * calibration_factor\n",
        "        \n",
        "        result[f'lo_{int(level*100)}'] = forecast_median - cal_w/2\n",
        "        result[f'hi_{int(level*100)}'] = forecast_median + cal_w/2\n",
        "    \n",
        "    result['bootstrap_forecasts'] = boot_forecasts\n",
        "    \n",
        "    return result\n",
        "\n",
        "\n",
        "# Generate calibrated forecast using selected model\n",
        "cal_factor = calibration_factors.get(selected_model, 1.0)\n",
        "\n",
        "future_years = np.arange(config.forecast_origin, config.forecast_origin + config.forecast_horizon + 1, 0.5)\n",
        "\n",
        "calibrated_forecast = generate_calibrated_forecast(\n",
        "    X, y,\n",
        "    MODEL_FITTERS[selected_model],\n",
        "    future_years,\n",
        "    calibration_factor=cal_factor,\n",
        "    n_bootstrap=config.n_bootstrap,\n",
        ")\n",
        "\n",
        "print(f\"\\nCalibrated Forecast ({selected_model} model):\")\n",
        "print(f\"  Calibration factor: {cal_factor:.2f}\")\n",
        "print(f\"  R²: {calibrated_forecast['r_squared']:.3f}\")\n",
        "\n",
        "# Handle different model parameter structures\n",
        "params = calibrated_forecast['model_params']\n",
        "if 'slope' in params:\n",
        "    print(f\"  Slope: {params['slope']:.3f} log₁₀FLOP/year\")\n",
        "    print(f\"  Growth rate: {10**params['slope']:.2f}x/year\")\n",
        "elif 'slope_after' in params:\n",
        "    print(f\"  Slope (recent): {params['slope_after']:.3f} log₁₀FLOP/year\")\n",
        "    print(f\"  Slope (early): {params.get('slope_before', 'N/A'):.3f} log₁₀FLOP/year\")\n",
        "    print(f\"  Changepoint: {params.get('changepoint', 'N/A'):.1f}\")\n",
        "    print(f\"  Growth rate (recent): {10**params['slope_after']:.2f}x/year\")\n",
        "\n",
        "print(f\"\\nForecast Summary:\")\n",
        "for i, yr in enumerate(future_years):\n",
        "    if yr == int(yr):  # Only show integer years\n",
        "        print(f\"  {int(yr)}: {calibrated_forecast['median'][i]:.2f} \"\n",
        "              f\"[{calibrated_forecast['lo_90'][i]:.2f}, {calibrated_forecast['hi_90'][i]:.2f}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: calibrated_forecast_final (png, pdf)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CALIBRATED FORECAST VISUALIZATION (with Threshold Taxonomy)\n",
        "# =============================================================================\n",
        "# Caption: \"Frontier AI compute forecast with two policy-defined thresholds \n",
        "# (EU AI Act 10²⁵, US EO 10²⁶) and three illustrative scenario markers for \n",
        "# forecasting visualization (not capability claims). Shaded regions show 50%, \n",
        "# 80%, 95% prediction intervals from bootstrap resampling.\"\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "# Historical data\n",
        "ax.scatter(X, y, s=100, c=COLORS['gold'], edgecolors='white', linewidth=2, \n",
        "           zorder=10, label='Historical frontier')\n",
        "\n",
        "# Prediction intervals (blue for compute)\n",
        "years = calibrated_forecast['years']\n",
        "median = calibrated_forecast['median']\n",
        "\n",
        "ax.fill_between(years, calibrated_forecast['lo_95'], calibrated_forecast['hi_95'], \n",
        "                alpha=0.15, color=COLORS['blue'], label='95% PI (calibrated)')\n",
        "ax.fill_between(years, calibrated_forecast['lo_80'], calibrated_forecast['hi_80'], \n",
        "                alpha=0.25, color=COLORS['blue'], label='80% PI (calibrated)')\n",
        "ax.fill_between(years, calibrated_forecast['lo_50'], calibrated_forecast['hi_50'], \n",
        "                alpha=0.4, color=COLORS['blue'], label='50% PI (calibrated)')\n",
        "\n",
        "ax.plot(years, median, color=COLORS['blue'], linewidth=2.5, label='Median forecast')\n",
        "\n",
        "# =============================================================================\n",
        "# THRESHOLDS: Policy-Defined (SOLID) vs Scenario Markers (DOTTED)\n",
        "# =============================================================================\n",
        "\n",
        "# Policy-defined thresholds (solid lines) - defensible\n",
        "for name, info in POLICY_DEFINED_THRESHOLDS.items():\n",
        "    ax.axhline(info['value'], color=info['color'], linestyle=info['line_style'], \n",
        "               linewidth=2, alpha=0.9)\n",
        "    ax.text(2029.7, info['value'] + 0.15, f\"■ {name}\", fontsize=8, \n",
        "            color=info['color'], ha='right', fontweight='bold')\n",
        "\n",
        "# Scenario markers (dotted lines) - NOT capability claims\n",
        "for name, info in SCENARIO_MARKERS.items():\n",
        "    ax.axhline(info['value'], color=info['color'], linestyle=info['line_style'], \n",
        "               linewidth=1.5, alpha=0.7)\n",
        "    ax.text(2029.7, info['value'] + 0.15, f\"⋯ {name}\", fontsize=8, \n",
        "            color=info['color'], ha='right', style='italic')\n",
        "\n",
        "# Vertical line at forecast origin\n",
        "ax.axvline(config.forecast_origin, color=COLORS['slate'], linestyle=':', alpha=0.6, linewidth=1.5)\n",
        "ax.text(config.forecast_origin + 0.1, 21, 'Forecast\\norigin', fontsize=9, color=COLORS['slate'])\n",
        "\n",
        "ax.set_xlabel('Year', fontsize=11)\n",
        "ax.set_ylabel('log₁₀(Training FLOP)', fontsize=11)\n",
        "ax.set_title(f'Calibrated Frontier Compute Forecast\\n(■ Policy thresholds | ⋯ Compute milestones)', fontsize=12)\n",
        "ax.legend(loc='upper left', fontsize=9)\n",
        "ax.set_xlim(2012, 2030)\n",
        "ax.set_ylim(17, 31)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add caption as text annotation\n",
        "caption = \"\"\"Figure caption: Frontier AI compute forecast with two policy-defined thresholds \n",
        "(EU AI Act 10²⁵, US EO 10²⁶) and three compute milestones anchored to the US EO threshold \n",
        "(10×, 100×, ~3000×). Shaded regions show 50%, 80%, 95% prediction intervals from \n",
        "bootstrap resampling with empirical recalibration.\"\"\"\n",
        "fig.text(0.5, -0.02, caption, ha='center', fontsize=8, style='italic', wrap=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure(fig, 'calibrated_forecast_final', OUTPUT_DIR, dpi=300)\n",
        "# plt.show()  # Disabled for batch execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Backtest Diagnostics for Selected Model\n",
        "\n",
        "**NEW section:** Reproduces the beautiful 3-panel backtest diagnostic figure from the hackathon version for the selected model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: selected_model_backtest (png, pdf)\n",
            "\n",
            "Backtest Summary (piecewise model, 1-year-ahead):\n",
            "  Number of forecasts: 7\n",
            "  MAE: 0.460 log₁₀FLOP\n",
            "  RMSE: 0.569 log₁₀FLOP\n",
            "  90% PI coverage: 85.7%\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# Section 10: Backtest Diagnostics for Selected Model\n",
        "# =============================================================================\n",
        "# Reproduce the backtest_results.png figure style for the selected model\n",
        "\n",
        "selected_model_name = selected_model\n",
        "selected_backtest = backtest_results[selected_model_name]\n",
        "\n",
        "# Filter to 1-year-ahead only for cleaner visualization\n",
        "one_year_ahead = [r for r in selected_backtest if r.horizon == 1]\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(plot_dims['width'] * 2.2, plot_dims['height']))\n",
        "\n",
        "# Panel 1: Actual vs Predicted\n",
        "ax = axes[0]\n",
        "actuals = [r.actual for r in one_year_ahead]\n",
        "preds = [r.predicted for r in one_year_ahead]\n",
        "ax.scatter(actuals, preds, s=60, c=COLORS['gold'], \n",
        "          edgecolors=COLORS['charcoal'], linewidth=0.5, alpha=0.8)\n",
        "lims = [min(actuals + preds) - 0.5, max(actuals + preds) + 0.5]\n",
        "ax.plot(lims, lims, color=COLORS['light_gray'], linestyle='--', \n",
        "        linewidth=1.5, label='Perfect prediction')\n",
        "ax.set_xlabel('Actual (log₁₀FLOP)')\n",
        "ax.set_ylabel('Predicted (log₁₀FLOP)')\n",
        "ax.set_title('Actual vs Predicted', fontweight='bold')\n",
        "ax.legend(frameon=False)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Panel 2: Errors over time\n",
        "ax = axes[1]\n",
        "years = [r.target_year for r in one_year_ahead]\n",
        "errors = [r.error for r in one_year_ahead]\n",
        "bar_colors = [COLORS['teal'] if r.covered else COLORS['amber'] for r in one_year_ahead]\n",
        "ax.bar(years, errors, color=bar_colors, alpha=0.8, \n",
        "       edgecolor=COLORS['charcoal'], linewidth=0.5)\n",
        "ax.axhline(y=0, color=COLORS['charcoal'], linestyle='-', linewidth=1)\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_ylabel('Forecast Error (log₁₀FLOP)')\n",
        "ax.set_title('Forecast Errors', fontweight='bold')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Panel 3: Prediction intervals\n",
        "ax = axes[2]\n",
        "for i, r in enumerate(one_year_ahead):\n",
        "    color = COLORS['teal'] if r.covered else COLORS['amber']\n",
        "    ax.errorbar(r.target_year, r.predicted, \n",
        "                yerr=[[r.predicted - r.pi_90_lo], [r.pi_90_hi - r.predicted]],\n",
        "                fmt='o', color=color, capsize=3, capthick=1.5, markersize=6,\n",
        "                markeredgecolor=COLORS['charcoal'], markeredgewidth=0.5)\n",
        "ax.scatter([r.target_year for r in one_year_ahead], \n",
        "           [r.actual for r in one_year_ahead], \n",
        "           marker='x', s=60, color=COLORS['charcoal'], label='Actual', \n",
        "           zorder=5, linewidth=1.5)\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_ylabel('log₁₀FLOP')\n",
        "ax.set_title('90% PI vs Actuals', fontweight='bold')\n",
        "ax.legend(frameon=False)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add color legend explanation\n",
        "fig.text(0.5, -0.02, 'Teal = covered by 90% PI | Amber = outside 90% PI', \n",
        "         ha='center', fontsize=8, style='italic')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure(fig, 'selected_model_backtest', OUTPUT_DIR, dpi=300)\n",
        "# plt.show()  # Disabled for batch execution\n",
        "\n",
        "# Print summary statistics\n",
        "print(f\"\\nBacktest Summary ({selected_model_name} model, 1-year-ahead):\")\n",
        "print(f\"  Number of forecasts: {len(one_year_ahead)}\")\n",
        "print(f\"  MAE: {np.mean([abs(r.error) for r in one_year_ahead]):.3f} log₁₀FLOP\")\n",
        "print(f\"  RMSE: {np.sqrt(np.mean([r.error**2 for r in one_year_ahead])):.3f} log₁₀FLOP\")\n",
        "print(f\"  90% PI coverage: {np.mean([r.covered for r in one_year_ahead]):.1%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Crossing Time Estimates (with Threshold Taxonomy)\n",
        "\n",
        "Estimate when frontier AI compute will cross each threshold.\n",
        "\n",
        "**Important distinction:**\n",
        "- **Policy-defined thresholds** have concrete policy implications\n",
        "- **Compute milestones** (10×, 100×, ~3000× US EO) are training budget forecasts, NOT capability claims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current frontier (historical max): 10^26.70 FLOP\n",
            "\n",
            "Threshold Crossing Estimates:\n",
            "==========================================================================================\n",
            "Threshold                      Type             log₁₀FLOP     Year Status              \n",
            "------------------------------------------------------------------------------------------\n",
            "EU AI Act Systemic Risk        POLICY                25.0   2022.5 ✓ Crossed ~2022     \n",
            "US EO 14110 Reporting          POLICY                26.0   2024.0 ✓ Crossed ~2024     \n",
            "10× US EO (10²⁷)               Scenario              27.0   2026.0 0.0y from now       \n",
            "100× US EO (10²⁸)              Scenario              28.0   2026.8 0.8y from now       \n",
            "~3000× US EO (10²⁹·⁵)          Scenario              29.5   2028.8 2.8y from now       \n",
            "\n",
            "Saved: output/crossing_estimates_final.csv\n"
          ]
        }
      ],
      "source": [
        "def estimate_crossing_times_calibrated(\n",
        "    calibrated_forecast: Dict,\n",
        "    thresholds: Dict[str, float],\n",
        "    threshold_metadata: Dict,  # NEW: includes line_style, color, etc.\n",
        "    forecast_origin: float,\n",
        "    historical_max: float,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Estimate crossing times from calibrated forecast.\n",
        "    \n",
        "    Fixed: Properly detects already-crossed thresholds by checking historical data.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    boot_forecasts = calibrated_forecast['bootstrap_forecasts']\n",
        "    years = calibrated_forecast['years']\n",
        "    \n",
        "    for name, threshold in thresholds.items():\n",
        "        # Get metadata if available\n",
        "        meta = threshold_metadata.get(name, {})\n",
        "        is_policy_defined = meta.get('source', None) is not None\n",
        "        \n",
        "        # Check if threshold was already crossed in historical data\n",
        "        already_crossed = threshold <= historical_max\n",
        "        \n",
        "        if already_crossed:\n",
        "            # Back-calculate when threshold was crossed using model params\n",
        "            params = calibrated_forecast['model_params']\n",
        "            if 'slope' in params:\n",
        "                slope = params['slope']\n",
        "                intercept = params['intercept']\n",
        "                crossing_year = (threshold - intercept) / slope\n",
        "            elif 'slope_after' in params:\n",
        "                # For piecewise: use the recent slope and work backwards\n",
        "                slope = params['slope_after']\n",
        "                cp = params.get('changepoint', 2018)\n",
        "                # Get the value at changepoint from the model\n",
        "                # Approximate: value at 2025 is historical_max, work back from there\n",
        "                # Calculate recent intercept from historical max and model\n",
        "                # historical_max is at the most recent year in our data\n",
        "                recent_year = 2025  # From our config.end_year\n",
        "                recent_intercept = historical_max - slope * recent_year\n",
        "                crossing_year = (threshold - recent_intercept) / slope\n",
        "            else:\n",
        "                crossing_year = forecast_origin - 3  # Fallback\n",
        "            \n",
        "            results.append({\n",
        "                'threshold': name,\n",
        "                'value_log_flop': threshold,\n",
        "                'crossing_median': crossing_year,\n",
        "                'crossing_lo_50': crossing_year,\n",
        "                'crossing_hi_50': crossing_year,\n",
        "                'crossing_lo_95': crossing_year,\n",
        "                'crossing_hi_95': crossing_year,\n",
        "                'years_from_origin': crossing_year - forecast_origin,\n",
        "                'already_crossed': True,\n",
        "                'is_policy_defined': is_policy_defined,\n",
        "            })\n",
        "            continue\n",
        "        \n",
        "        # Bootstrap crossing times for future thresholds\n",
        "        crossing_times = []\n",
        "        for b in range(boot_forecasts.shape[0]):\n",
        "            traj = boot_forecasts[b]\n",
        "            crossed_idx = np.where(traj >= threshold)[0]\n",
        "            if len(crossed_idx) > 0:\n",
        "                idx = crossed_idx[0]\n",
        "                if idx > 0:\n",
        "                    y0, y1 = traj[idx-1], traj[idx]\n",
        "                    t0, t1 = years[idx-1], years[idx]\n",
        "                    if y1 > y0:\n",
        "                        cross_time = t0 + (threshold - y0) / (y1 - y0) * (t1 - t0)\n",
        "                    else:\n",
        "                        cross_time = t0\n",
        "                else:\n",
        "                    cross_time = years[idx]\n",
        "                crossing_times.append(cross_time)\n",
        "        \n",
        "        if not crossing_times:\n",
        "            crossing_times = [years[-1] + 5]  # Beyond forecast horizon\n",
        "        \n",
        "        crossing_times = np.array(crossing_times)\n",
        "        median_crossing = np.median(crossing_times)\n",
        "        \n",
        "        results.append({\n",
        "            'threshold': name,\n",
        "            'value_log_flop': threshold,\n",
        "            'crossing_median': median_crossing,\n",
        "            'crossing_lo_50': np.percentile(crossing_times, 25),\n",
        "            'crossing_hi_50': np.percentile(crossing_times, 75),\n",
        "            'crossing_lo_95': np.percentile(crossing_times, 2.5),\n",
        "            'crossing_hi_95': np.percentile(crossing_times, 97.5),\n",
        "            'years_from_origin': median_crossing - forecast_origin,\n",
        "            'already_crossed': False,\n",
        "            'is_policy_defined': is_policy_defined,\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# Combine threshold metadata\n",
        "threshold_metadata = {\n",
        "    **{name: info for name, info in POLICY_DEFINED_THRESHOLDS.items()},\n",
        "    **{name: info for name, info in SCENARIO_MARKERS.items()},\n",
        "}\n",
        "\n",
        "# Compute crossing times\n",
        "historical_max = frontier_df['log_flop'].max()\n",
        "print(f\"Current frontier (historical max): 10^{historical_max:.2f} FLOP\")\n",
        "\n",
        "crossing_df = estimate_crossing_times_calibrated(\n",
        "    calibrated_forecast,\n",
        "    ALL_THRESHOLDS,\n",
        "    threshold_metadata,\n",
        "    config.forecast_origin,\n",
        "    historical_max,\n",
        ")\n",
        "\n",
        "print(\"\\nThreshold Crossing Estimates:\")\n",
        "print(\"=\" * 90)\n",
        "print(f\"{'Threshold':<30} {'Type':<15} {'log₁₀FLOP':>10} {'Year':>8} {'Status':<20}\")\n",
        "print(\"-\" * 90)\n",
        "\n",
        "for _, row in crossing_df.sort_values('value_log_flop').iterrows():\n",
        "    type_str = \"POLICY\" if row['is_policy_defined'] else \"Scenario\"\n",
        "    if row['already_crossed']:\n",
        "        status = f\"✓ Crossed ~{row['crossing_median']:.0f}\"\n",
        "    else:\n",
        "        status = f\"{row['years_from_origin']:.1f}y from now\"\n",
        "    print(f\"{row['threshold']:<30} {type_str:<15} {row['value_log_flop']:>10.1f} \"\n",
        "          f\"{row['crossing_median']:>8.1f} {status:<20}\")\n",
        "\n",
        "# Save crossing estimates\n",
        "crossing_df.to_csv(OUTPUT_DIR / 'crossing_estimates_final.csv', index=False)\n",
        "print(f\"\\nSaved: output/crossing_estimates_final.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: crossing_timeline_final (png, pdf)\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CROSSING TIMELINE VISUALIZATION (with Threshold Taxonomy)\n",
        "# =============================================================================\n",
        "# Caption: \"Timeline: Estimated crossing dates for policy-defined thresholds \n",
        "# and illustrative scenario markers. Policy thresholds (solid bars) are \n",
        "# defensible triggers. Scenario markers (hatched bars) are forecasting milestones, \n",
        "# not capability emergence claims.\"\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "y_pos = np.arange(len(crossing_df))\n",
        "crossing_sorted = crossing_df.sort_values('value_log_flop')\n",
        "\n",
        "for i, (_, row) in enumerate(crossing_sorted.iterrows()):\n",
        "    if row['already_crossed']:\n",
        "        # Already crossed - green solid bar\n",
        "        ax.barh(i, 0.15, left=row['crossing_median']-0.075, \n",
        "                color=COLORS['green'], alpha=0.9, height=0.6, \n",
        "                edgecolor=COLORS['charcoal'], linewidth=0.5)\n",
        "        ax.text(row['crossing_median'] + 0.3, i, f\"✓ {row['crossing_median']:.0f}\", \n",
        "                va='center', fontsize=9, color=COLORS['green'], fontweight='bold')\n",
        "    else:\n",
        "        if row['is_policy_defined']:\n",
        "            # Policy threshold - solid blue bar\n",
        "            bar_color = COLORS['blue']\n",
        "            hatch = None\n",
        "            alpha_outer = 0.4\n",
        "            alpha_inner = 0.7\n",
        "        else:\n",
        "            # Scenario marker - hatched warm color bar\n",
        "            # Color based on threshold value\n",
        "            if row['value_log_flop'] <= 27:\n",
        "                bar_color = COLORS['gold']\n",
        "            elif row['value_log_flop'] <= 28:\n",
        "                bar_color = COLORS['amber']\n",
        "            else:\n",
        "                bar_color = COLORS['orange']\n",
        "            hatch = '///'\n",
        "            alpha_outer = 0.25\n",
        "            alpha_inner = 0.5\n",
        "        \n",
        "        # 95% CI outer bar\n",
        "        ax.barh(i, row['crossing_hi_95'] - row['crossing_lo_95'], \n",
        "                left=row['crossing_lo_95'], color=bar_color, alpha=alpha_outer, \n",
        "                height=0.6, hatch=hatch, edgecolor=COLORS['charcoal'], linewidth=0.5)\n",
        "        # 50% CI inner bar\n",
        "        ax.barh(i, row['crossing_hi_50'] - row['crossing_lo_50'], \n",
        "                left=row['crossing_lo_50'], color=bar_color, alpha=alpha_inner, \n",
        "                height=0.4, hatch=hatch)\n",
        "        # Median marker\n",
        "        ax.plot(row['crossing_median'], i, 'o', color=bar_color, markersize=10, \n",
        "                markeredgecolor='white', markeredgewidth=1.5)\n",
        "\n",
        "# Y-axis labels with type indicator\n",
        "labels = []\n",
        "for _, row in crossing_sorted.iterrows():\n",
        "    prefix = \"■ \" if row['is_policy_defined'] else \"⋯ \"\n",
        "    labels.append(prefix + row['threshold'])\n",
        "\n",
        "ax.set_yticks(y_pos)\n",
        "ax.set_yticklabels(labels, fontsize=9)\n",
        "\n",
        "# Vertical line at forecast origin\n",
        "ax.axvline(config.forecast_origin, color=COLORS['red'], linestyle='--', \n",
        "           linewidth=2, label='Forecast origin (Jan 2026)')\n",
        "\n",
        "ax.set_xlabel('Year', fontsize=11)\n",
        "ax.set_title('Threshold Crossing Timeline\\n(■ Policy thresholds | ⋯ Compute milestones)', fontsize=12)\n",
        "ax.set_xlim(2020, 2032)\n",
        "ax.legend(loc='lower right', fontsize=9)\n",
        "ax.grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# Caption\n",
        "caption = \"\"\"Figure caption: Estimated crossing dates for policy-defined thresholds and \n",
        "compute milestones (10×, 100×, ~3000× US EO). Policy thresholds (solid bars) are defensible \n",
        "regulatory triggers. Compute milestones (hatched bars) are training budget forecasts.\"\"\"\n",
        "fig.text(0.5, -0.02, caption, ha='center', fontsize=8, style='italic', wrap=True)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure(fig, 'crossing_timeline_final', OUTPUT_DIR, dpi=300)\n",
        "# plt.show()  # Disabled for batch execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Capability Forecasting (Epoch Capabilities Index)\n",
        "\n",
        "Complementary forecasting using the Epoch Capabilities Index (ECI) - a multi-benchmark composite score that provides a more direct measure of actual model performance than training compute alone.\n",
        "\n",
        "**Key advantage:** R² = 0.84 (vs. 0.11 for HuggingFace Hub score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded ECI data: 277 models, 3 frontier points\n",
            "ECI range: 126.07 - 153.81\n",
            "Year range: 2024 - 2025\n",
            "\n",
            "ECI Linear Model:\n",
            "  R²: 0.991\n",
            "  Slope: 14.632 ECI/year\n",
            "\n",
            "ECI Forecast:\n",
            "  2026: 162.18 [157.58, 166.79]\n",
            "  2027: 176.62 [171.24, 182.00]\n",
            "  2028: 191.43 [184.73, 198.13]\n",
            "  2029: 205.96 [197.43, 214.49]\n"
          ]
        }
      ],
      "source": [
        "# Load ECI data\n",
        "eci_path = DATA_DIR / \"benchmark_data\" / \"epoch_capabilities_index.csv\"\n",
        "\n",
        "if eci_path.exists():\n",
        "    eci_df = pd.read_csv(eci_path)\n",
        "    \n",
        "    # Check columns and parse date\n",
        "    if 'Release date' in eci_df.columns:\n",
        "        eci_df['date'] = pd.to_datetime(eci_df['Release date'], errors='coerce')\n",
        "    elif 'date' in eci_df.columns:\n",
        "        eci_df['date'] = pd.to_datetime(eci_df['date'], errors='coerce')\n",
        "    \n",
        "    # Get ECI score\n",
        "    if 'ECI Score' in eci_df.columns:\n",
        "        eci_df['eci'] = eci_df['ECI Score']\n",
        "    elif 'eci' in eci_df.columns:\n",
        "        pass  # Already has 'eci'\n",
        "    \n",
        "    eci_df['year'] = eci_df['date'].dt.year + eci_df['date'].dt.dayofyear / 365.25\n",
        "    eci_df = eci_df[eci_df['eci'].notna()]\n",
        "    eci_df = eci_df[(eci_df['year'] >= 2020) & (eci_df['year'] <= 2026)]\n",
        "    \n",
        "    # Create frontier (max ECI per MONTH for more granularity)\n",
        "    eci_df['year_month'] = eci_df['date'].dt.to_period('M')\n",
        "    eci_frontier = eci_df.groupby('year_month').agg({\n",
        "        'eci': 'max',\n",
        "        'year': 'mean',\n",
        "    }).reset_index(drop=True)\n",
        "    eci_frontier.columns = ['eci', 'year']\n",
        "    eci_frontier = eci_frontier.sort_values('year').dropna()\n",
        "    \n",
        "    print(f\"Loaded ECI data: {len(eci_df)} models, {len(eci_frontier)} frontier points\")\n",
        "    print(f\"ECI range: {eci_frontier['eci'].min():.2f} - {eci_frontier['eci'].max():.2f}\")\n",
        "    print(f\"Year range: {eci_frontier['year'].min():.0f} - {eci_frontier['year'].max():.0f}\")\n",
        "    \n",
        "    # Fit linear model\n",
        "    X_eci = eci_frontier['year'].values.astype(float)\n",
        "    y_eci = eci_frontier['eci'].values\n",
        "    \n",
        "    eci_model = fit_linear(X_eci, y_eci)\n",
        "    print(f\"\\nECI Linear Model:\")\n",
        "    print(f\"  R²: {eci_model.r_squared:.3f}\")\n",
        "    print(f\"  Slope: {eci_model.params['slope']:.3f} ECI/year\")\n",
        "    \n",
        "    # Generate ECI forecast\n",
        "    eci_future_years = np.arange(2026, 2030, 0.5)\n",
        "    eci_calibrated_forecast = generate_calibrated_forecast(\n",
        "        X_eci, y_eci,\n",
        "        fit_linear,\n",
        "        eci_future_years,\n",
        "        calibration_factor=1.2,  # Slightly wider PIs due to less data\n",
        "        n_bootstrap=1000,\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nECI Forecast:\")\n",
        "    for i, yr in enumerate(eci_future_years):\n",
        "        if yr == int(yr):\n",
        "            print(f\"  {int(yr)}: {eci_calibrated_forecast['median'][i]:.2f} \"\n",
        "                  f\"[{eci_calibrated_forecast['lo_90'][i]:.2f}, \"\n",
        "                  f\"{eci_calibrated_forecast['hi_90'][i]:.2f}]\")\n",
        "else:\n",
        "    print(\"ECI data not found - skipping capability forecast\")\n",
        "    eci_frontier = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: eci_forecast (png, pdf)\n"
          ]
        }
      ],
      "source": [
        "# ECI Forecast Visualization (matching palette)\n",
        "if eci_path.exists():\n",
        "    fig, ax = plt.subplots(figsize=(12, 7))\n",
        "    \n",
        "    # Historical data\n",
        "    ax.scatter(X_eci, y_eci, s=100, c=COLORS['teal'], edgecolors='white', \n",
        "               linewidth=2, zorder=10, label='Historical ECI frontier')\n",
        "    \n",
        "    # Prediction intervals (teal for capability)\n",
        "    years = eci_calibrated_forecast['years']\n",
        "    median = eci_calibrated_forecast['median']\n",
        "    \n",
        "    ax.fill_between(years, eci_calibrated_forecast['lo_95'], \n",
        "                    eci_calibrated_forecast['hi_95'], \n",
        "                    alpha=0.15, color=COLORS['teal'], label='95% PI')\n",
        "    ax.fill_between(years, eci_calibrated_forecast['lo_80'], \n",
        "                    eci_calibrated_forecast['hi_80'], \n",
        "                    alpha=0.25, color=COLORS['teal'], label='80% PI')\n",
        "    ax.fill_between(years, eci_calibrated_forecast['lo_50'], \n",
        "                    eci_calibrated_forecast['hi_50'], \n",
        "                    alpha=0.4, color=COLORS['teal'], label='50% PI')\n",
        "    \n",
        "    ax.plot(years, median, color=COLORS['teal'], linewidth=2.5, \n",
        "            label='Median forecast')\n",
        "    \n",
        "    # Capability thresholds (illustrative - based on current frontier ~154 ECI)\n",
        "    ECI_THRESHOLDS = {\n",
        "        'PhD-Level Problem Solving': 165,\n",
        "        'Expert Mathematician (IMO Silver)': 180,\n",
        "        'Autonomous Research Agent': 195,\n",
        "        'Human-Expert Parity': 210,\n",
        "    }\n",
        "    \n",
        "    for name, value in ECI_THRESHOLDS.items():\n",
        "        ax.axhline(value, color=COLORS['amber'], linestyle=':', \n",
        "                   linewidth=1.5, alpha=0.7)\n",
        "        ax.text(2029.5, value + 1.5, name, fontsize=8, color=COLORS['amber'], \n",
        "                ha='right', style='italic')\n",
        "    \n",
        "    # Vertical line at forecast origin\n",
        "    ax.axvline(2026, color=COLORS['slate'], linestyle=':', alpha=0.6, linewidth=1.5)\n",
        "    ax.text(2026.1, y_eci.min() - 5, 'Forecast\\norigin', fontsize=9, color=COLORS['slate'])\n",
        "    \n",
        "    ax.set_xlabel('Year', fontsize=11)\n",
        "    ax.set_ylabel('Epoch Capabilities Index (ECI)', fontsize=11)\n",
        "    ax.set_title('Capability Forecast: Epoch Capabilities Index\\n(R² = {:.2f})'.format(\n",
        "        eci_model.r_squared), fontsize=12)\n",
        "    ax.legend(loc='upper left', fontsize=9)\n",
        "    ax.set_xlim(2023, 2030)\n",
        "    # Dynamic y-axis based on data and forecast\n",
        "    y_min = min(y_eci.min(), 120) - 10\n",
        "    y_max = max(eci_calibrated_forecast['hi_95'].max(), 210) + 10\n",
        "    ax.set_ylim(y_min, y_max)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_figure(fig, 'eci_forecast', OUTPUT_DIR, dpi=300)\n",
        "    # plt.show()  # Disabled for batch execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Robustness Checks\n",
        "\n",
        "Test sensitivity to key assumptions:\n",
        "1. Alternative frontier definitions (top-3 models per year)\n",
        "2. Different start years\n",
        "3. Growth rate sensitivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Robustness Check 1: Top-3 Frontier Definition\n",
            "--------------------------------------------------\n",
            "  Slope (top-1 frontier): 0.637\n",
            "  Slope (top-3 frontier): 0.661\n",
            "  Difference: 0.024\n",
            "\n",
            "Robustness Check 2: Start Year Sensitivity\n",
            "--------------------------------------------------\n",
            "  Start 2010: slope=0.637, R²=0.966, n=14\n",
            "  Start 2012: slope=0.637, R²=0.966, n=14\n",
            "  Start 2015: slope=0.602, R²=0.959, n=11\n",
            "  Start 2017: slope=0.671, R²=0.972, n=9\n",
            "\n",
            "Robustness Check 3: Growth Rate Uncertainty\n",
            "--------------------------------------------------\n",
            "  Point estimate: 0.637 log₁₀FLOP/year\n",
            "  95% CI: [0.570, 0.705]\n",
            "  Compute growth: 4.34x/year [3.72x, 5.07x]\n"
          ]
        }
      ],
      "source": [
        "# Robustness check 1: Top-3 frontier (mean of top 3 models per year)\n",
        "print(\"Robustness Check 1: Top-3 Frontier Definition\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "top3_frontier = df.groupby(df['year'].astype(int)).apply(\n",
        "    lambda g: g.nlargest(3, 'log_flop')['log_flop'].mean()\n",
        ").reset_index()\n",
        "top3_frontier.columns = ['year', 'log_flop']\n",
        "\n",
        "X_top3 = top3_frontier['year'].values.astype(float)\n",
        "y_top3 = top3_frontier['log_flop'].values\n",
        "\n",
        "top3_model = fit_linear(X_top3, y_top3)\n",
        "print(f\"  Slope (top-1 frontier): {models['linear'].params['slope']:.3f}\")\n",
        "print(f\"  Slope (top-3 frontier): {top3_model.params['slope']:.3f}\")\n",
        "print(f\"  Difference: {abs(models['linear'].params['slope'] - top3_model.params['slope']):.3f}\")\n",
        "\n",
        "# Robustness check 2: Different start years\n",
        "print(\"\\nRobustness Check 2: Start Year Sensitivity\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "start_years = [2010, 2012, 2015, 2017]\n",
        "for start_yr in start_years:\n",
        "    subset = frontier_df[frontier_df['year'] >= start_yr]\n",
        "    X_sub = subset['year'].values.astype(float)\n",
        "    y_sub = subset['log_flop'].values\n",
        "    if len(X_sub) < 5:\n",
        "        continue\n",
        "    sub_model = fit_linear(X_sub, y_sub)\n",
        "    print(f\"  Start {start_yr}: slope={sub_model.params['slope']:.3f}, \"\n",
        "          f\"R²={sub_model.r_squared:.3f}, n={len(X_sub)}\")\n",
        "\n",
        "# Robustness check 3: Growth rate confidence interval\n",
        "print(\"\\nRobustness Check 3: Growth Rate Uncertainty\")\n",
        "print(\"-\" * 50)\n",
        "slope = models['linear'].params['slope']\n",
        "slope_se = models['linear'].residual_std / np.sqrt(np.sum((X - X.mean())**2))\n",
        "slope_ci = (slope - 1.96 * slope_se, slope + 1.96 * slope_se)\n",
        "print(f\"  Point estimate: {slope:.3f} log₁₀FLOP/year\")\n",
        "print(f\"  95% CI: [{slope_ci[0]:.3f}, {slope_ci[1]:.3f}]\")\n",
        "print(f\"  Compute growth: {10**slope:.2f}x/year [{10**slope_ci[0]:.2f}x, {10**slope_ci[1]:.2f}x]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: robustness_checks (png, pdf)\n"
          ]
        }
      ],
      "source": [
        "# Visualize robustness: Slope sensitivity\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Left: Frontier definition comparison\n",
        "ax = axes[0]\n",
        "ax.scatter(X, y, s=80, c=COLORS['gold'], edgecolors='white', linewidth=1.5, \n",
        "           label='Top-1 frontier', zorder=5)\n",
        "ax.scatter(X_top3, y_top3, s=60, c=COLORS['teal'], edgecolors='white', \n",
        "           linewidth=1, label='Top-3 mean', alpha=0.7, zorder=4)\n",
        "\n",
        "x_range = np.linspace(2010, 2030, 100)\n",
        "ax.plot(x_range, models['linear'].predict_func(x_range), \n",
        "        color=COLORS['gold'], linewidth=2, label=f'Top-1 fit (slope={models[\"linear\"].params[\"slope\"]:.3f})')\n",
        "ax.plot(x_range, top3_model.predict_func(x_range), \n",
        "        color=COLORS['teal'], linewidth=2, linestyle='--', \n",
        "        label=f'Top-3 fit (slope={top3_model.params[\"slope\"]:.3f})')\n",
        "\n",
        "ax.set_xlabel('Year', fontsize=11)\n",
        "ax.set_ylabel('log₁₀(Training FLOP)', fontsize=11)\n",
        "ax.set_title('Frontier Definition Sensitivity', fontsize=12)\n",
        "ax.legend(loc='upper left', fontsize=9)\n",
        "ax.set_xlim(2010, 2030)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Right: Start year sensitivity\n",
        "ax = axes[1]\n",
        "slopes = []\n",
        "r2s = []\n",
        "for start_yr in range(2010, 2020):\n",
        "    subset = frontier_df[frontier_df['year'] >= start_yr]\n",
        "    if len(subset) < 5:\n",
        "        continue\n",
        "    X_sub = subset['year'].values.astype(float)\n",
        "    y_sub = subset['log_flop'].values\n",
        "    sub_model = fit_linear(X_sub, y_sub)\n",
        "    slopes.append({'start': start_yr, 'slope': sub_model.params['slope'], \n",
        "                   'r2': sub_model.r_squared})\n",
        "\n",
        "slopes_df = pd.DataFrame(slopes)\n",
        "ax.bar(slopes_df['start'], slopes_df['slope'], color=COLORS['blue'], alpha=0.7,\n",
        "       edgecolor=COLORS['charcoal'], linewidth=0.5)\n",
        "ax.axhline(models['linear'].params['slope'], color=COLORS['red'], linestyle='--',\n",
        "           linewidth=1.5, label=f'Selected (2012): {models[\"linear\"].params[\"slope\"]:.3f}')\n",
        "\n",
        "ax.set_xlabel('Start Year', fontsize=11)\n",
        "ax.set_ylabel('Slope (log₁₀FLOP/year)', fontsize=11)\n",
        "ax.set_title('Start Year Sensitivity', fontsize=12)\n",
        "ax.legend(loc='upper right', fontsize=9)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "save_figure(fig, 'robustness_checks', OUTPUT_DIR, dpi=300)\n",
        "# plt.show()  # Disabled for batch execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Summary Statistics & Export\n",
        "\n",
        "Final summary of results and export to JSON/CSV for paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "SUMMARY STATISTICS FOR SUBMISSION\n",
            "======================================================================\n",
            "\n",
            "1. DATA\n",
            "   Source: Epoch AI Notable AI Models\n",
            "   Frontier points: 14\n",
            "   Period: 2012 - 2025\n",
            "   Current frontier: 10^26.70 FLOP\n",
            "\n",
            "2. MODEL SELECTION\n",
            "   Selected: piecewise\n",
            "   R²: 0.985\n",
            "   Slope: 0.671 log₁₀FLOP/year\n",
            "   Growth rate: 4.69x/year\n",
            "\n",
            "3. CALIBRATION\n",
            "   Factor applied: 1.15\n",
            "   Raw 90% coverage: 88.9%\n",
            "   Target coverage: 90%\n",
            "\n",
            "4. THRESHOLD TAXONOMY\n",
            "   Policy-Defined (solid lines):\n",
            "     EU AI Act Systemic Risk: 10^25 FLOP\n",
            "     US EO 14110 Reporting: 10^26 FLOP\n",
            "   Compute Milestones (dotted lines - anchored to US EO):\n",
            "     10× US EO (10²⁷): 10^27.0 FLOP\n",
            "     100× US EO (10²⁸): 10^28.0 FLOP\n",
            "     ~3000× US EO (10²⁹·⁵): 10^29.5 FLOP\n",
            "\n",
            "5. CROSSING ESTIMATES\n",
            "   ✓ EU AI Act Systemic Risk: ~2022 (crossed)\n",
            "   ✓ US EO 14110 Reporting: ~2024 (crossed)\n",
            "   → 10× US EO (10²⁷): 2026.0 [2026.0, 2026.5] (Milestone)\n",
            "   → 100× US EO (10²⁸): 2026.8 [2026.0, 2028.0] (Milestone)\n",
            "   → ~3000× US EO (10²⁹·⁵): 2028.8 [2027.8, 2029.5] (Milestone)\n",
            "\n",
            "6. LIMITATIONS (Critical Caveat)\n",
            "   - Only 2 thresholds are policy-defined (EU AI Act, US EO 14110)\n",
            "   - Compute milestones are training budget forecasts, NOT capability claims\n",
            "   - Compute is an imperfect proxy for capability (Erben et al. 2025)\n",
            "   - FLOP forecasts estimate training budgets, NOT capability emergence\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"SUMMARY STATISTICS FOR SUBMISSION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\n1. DATA\")\n",
        "print(f\"   Source: Epoch AI Notable AI Models\")\n",
        "print(f\"   Frontier points: {len(frontier_df)}\")\n",
        "print(f\"   Period: {config.start_year} - {config.end_year}\")\n",
        "print(f\"   Current frontier: 10^{frontier_df['log_flop'].max():.2f} FLOP\")\n",
        "\n",
        "print(\"\\n2. MODEL SELECTION\")\n",
        "print(f\"   Selected: {selected_model}\")\n",
        "print(f\"   R²: {models[selected_model].r_squared:.3f}\")\n",
        "slope_val = models[selected_model].params.get('slope', models[selected_model].params.get('slope_after', None))\n",
        "if slope_val is not None:\n",
        "    print(f\"   Slope: {slope_val:.3f} log₁₀FLOP/year\")\n",
        "else:\n",
        "    print(f\"   Slope: N/A\")\n",
        "if slope_val is not None:\n",
        "    print(f\"   Growth rate: {10**slope_val:.2f}x/year\")\n",
        "\n",
        "print(\"\\n3. CALIBRATION\")\n",
        "print(f\"   Factor applied: {cal_factor:.2f}\")\n",
        "linear_results = backtest_results.get(selected_model, [])\n",
        "if linear_results:\n",
        "    print(f\"   Raw 90% coverage: {compute_coverage(linear_results, 0.90):.1%}\")\n",
        "    print(f\"   Target coverage: 90%\")\n",
        "\n",
        "print(\"\\n4. THRESHOLD TAXONOMY\")\n",
        "print(\"   Policy-Defined (solid lines):\")\n",
        "for name, info in POLICY_DEFINED_THRESHOLDS.items():\n",
        "    print(f\"     {name}: 10^{info['value']:.0f} FLOP\")\n",
        "print(\"   Compute Milestones (dotted lines - anchored to US EO):\")\n",
        "for name, info in SCENARIO_MARKERS.items():\n",
        "    print(f\"     {name}: 10^{info['value']:.1f} FLOP\")\n",
        "\n",
        "print(\"\\n5. CROSSING ESTIMATES\")\n",
        "for _, row in crossing_df.sort_values('value_log_flop').iterrows():\n",
        "    type_str = \"POLICY\" if row['is_policy_defined'] else \"Milestone\"\n",
        "    if row['already_crossed']:\n",
        "        print(f\"   ✓ {row['threshold']}: ~{row['crossing_median']:.0f} (crossed)\")\n",
        "    else:\n",
        "        print(f\"   → {row['threshold']}: {row['crossing_median']:.1f} \"\n",
        "              f\"[{row['crossing_lo_95']:.1f}, {row['crossing_hi_95']:.1f}] ({type_str})\")\n",
        "\n",
        "print(\"\\n6. LIMITATIONS (Critical Caveat)\")\n",
        "print(\"   - Only 2 thresholds are policy-defined (EU AI Act, US EO 14110)\")\n",
        "print(\"   - Compute milestones are training budget forecasts, NOT capability claims\")\n",
        "print(\"   - Compute is an imperfect proxy for capability (Erben et al. 2025)\")\n",
        "print(\"   - FLOP forecasts estimate training budgets, NOT capability emergence\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported results:\n",
            "  - output/submission_results.json\n",
            "  - output/crossing_estimates_final.csv\n",
            "  - output/model_comparison.csv\n",
            "  - output/forecast_results_final.csv\n"
          ]
        }
      ],
      "source": [
        "# Export all results\n",
        "results_summary = {\n",
        "    'metadata': {\n",
        "        'generated_at': datetime.now().isoformat(),\n",
        "        'data_source': 'Epoch AI Notable AI Models',\n",
        "        'forecast_origin': config.forecast_origin,\n",
        "        'forecast_horizon': config.forecast_horizon,\n",
        "    },\n",
        "    'config': {\n",
        "        'start_year': config.start_year,\n",
        "        'end_year': config.end_year,\n",
        "        'n_bootstrap': config.n_bootstrap,\n",
        "        'backtest_horizons': list(config.backtest_horizons),\n",
        "    },\n",
        "    'model': {\n",
        "        'name': selected_model,\n",
        "        'r_squared': float(models[selected_model].r_squared),\n",
        "        'slope': float(models[selected_model].params.get('slope', 0)),\n",
        "        'intercept': float(models[selected_model].params.get('intercept', 0)),\n",
        "        'calibration_factor': float(cal_factor),\n",
        "    },\n",
        "    'threshold_taxonomy': {\n",
        "        'policy_defined': {name: info['value'] for name, info in POLICY_DEFINED_THRESHOLDS.items()},\n",
        "        'scenario_markers': {name: info['value'] for name, info in SCENARIO_MARKERS.items()},\n",
        "        'note': 'Only policy_defined thresholds are defensible. Scenario markers are for forecasting only.',\n",
        "    },\n",
        "    'crossing_estimates': crossing_df.to_dict('records'),\n",
        "    'forecast': {\n",
        "        'years': [float(y) for y in calibrated_forecast['years']],\n",
        "        'median': [float(m) for m in calibrated_forecast['median']],\n",
        "        'lo_90': [float(l) for l in calibrated_forecast['lo_90']],\n",
        "        'hi_90': [float(h) for h in calibrated_forecast['hi_90']],\n",
        "    },\n",
        "    'backtest_performance': {\n",
        "        'model': selected_model,\n",
        "        'n_forecasts': len(linear_results) if linear_results else 0,\n",
        "        'mae': float(np.mean([abs(r.error) for r in linear_results])) if linear_results else None,\n",
        "        'rmse': float(np.sqrt(np.mean([r.error**2 for r in linear_results]))) if linear_results else None,\n",
        "        'coverage_90': float(compute_coverage(linear_results, 0.90)) if linear_results else None,\n",
        "    },\n",
        "    'model_comparison': comparison_df.to_dict('records'),\n",
        "}\n",
        "\n",
        "# Save JSON\n",
        "with open(OUTPUT_DIR / 'submission_results.json', 'w') as f:\n",
        "    json.dump(results_summary, f, indent=2, default=str)\n",
        "\n",
        "# Save CSVs\n",
        "crossing_df.to_csv(OUTPUT_DIR / 'crossing_estimates_final.csv', index=False)\n",
        "comparison_df.to_csv(OUTPUT_DIR / 'model_comparison.csv', index=False)\n",
        "\n",
        "# Save forecast\n",
        "forecast_df = pd.DataFrame({\n",
        "    'year': calibrated_forecast['years'],\n",
        "    'median': calibrated_forecast['median'],\n",
        "    'lo_50': calibrated_forecast['lo_50'],\n",
        "    'hi_50': calibrated_forecast['hi_50'],\n",
        "    'lo_80': calibrated_forecast['lo_80'],\n",
        "    'hi_80': calibrated_forecast['hi_80'],\n",
        "    'lo_90': calibrated_forecast['lo_90'],\n",
        "    'hi_90': calibrated_forecast['hi_90'],\n",
        "    'lo_95': calibrated_forecast['lo_95'],\n",
        "    'hi_95': calibrated_forecast['hi_95'],\n",
        "})\n",
        "forecast_df.to_csv(OUTPUT_DIR / 'forecast_results_final.csv', index=False)\n",
        "\n",
        "print(\"Exported results:\")\n",
        "print(f\"  - output/submission_results.json\")\n",
        "print(f\"  - output/crossing_estimates_final.csv\")\n",
        "print(f\"  - output/model_comparison.csv\")\n",
        "print(f\"  - output/forecast_results_final.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Limitations\n",
        "\n",
        "### 15.1 Threshold Validity\n",
        "\n",
        "**Critical caveat:** Only 2 of our thresholds are policy-defined (EU AI Act 10²⁵, US EO 10²⁶). The remaining thresholds are **illustrative scenario markers** for forecasting purposes, not claims about capability emergence.\n",
        "\n",
        "As Erben et al. (2025) demonstrate:\n",
        "> \"Compute only weakly correlates with aggregated task performance and is not a reliable predictor for individual task performance.\"\n",
        "\n",
        "Our FLOP-based forecasts estimate **when training budgets will grow**, not when specific capabilities will emerge. The capability thresholds (ECI-based) provide a complementary but still imperfect measure of actual performance trends.\n",
        "\n",
        "### 15.2 Compute as Proxy\n",
        "\n",
        "Training compute is an imperfect proxy for capability:\n",
        "- Algorithmic efficiency can decouple compute from capability over time\n",
        "- Data quality, architecture, and post-training matter significantly\n",
        "- Scaling laws may plateau or break down\n",
        "\n",
        "### 15.3 Extrapolation Uncertainty\n",
        "\n",
        "- 3-year forecasts at the frontier carry substantial uncertainty\n",
        "- Historical trends may not persist (regime changes, compute constraints)\n",
        "- Prediction intervals widen significantly with horizon\n",
        "\n",
        "### 15.4 Data Limitations\n",
        "\n",
        "- Epoch AI data may not capture all frontier models (proprietary systems)\n",
        "- Publication dates may not reflect actual training completion dates\n",
        "- Compute estimates for recent models have higher uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "NOTEBOOK COMPLETE\n",
            "======================================================================\n",
            "\n",
            "Generated outputs:\n",
            "  Figures:\n",
            "    - calibrated_forecast_final.png/pdf (Main result)\n",
            "    - crossing_timeline_final.png/pdf (Key finding)\n",
            "    - model_comparison.png/pdf (Methodology)\n",
            "    - calibration_analysis.png/pdf (Validation)\n",
            "    - selected_model_backtest.png/pdf (Diagnostics)\n",
            "    - eci_forecast.png/pdf (Capability forecast)\n",
            "    - robustness_checks.png/pdf (Sensitivity)\n",
            "\n",
            "  Data:\n",
            "    - submission_results.json (All results)\n",
            "    - crossing_estimates_final.csv\n",
            "    - model_comparison.csv\n",
            "    - forecast_results_final.csv\n",
            "\n",
            "Key findings:\n",
            "  1. Frontier compute growth: ~4.69x/year (95% CI: [4.19x, 5.25x])\n",
            "  2. EU AI Act threshold (10²⁵): Already crossed\n",
            "  3. US EO threshold (10²⁶): Already crossed  \n",
            "  4. 10× US EO (10²⁷): Expected ~2026\n",
            "  5. 100× US EO (10²⁸): Expected ~2027\n",
            "  6. ~3000× US EO (10²⁹·⁵): Expected ~2028-2029\n",
            "\n",
            "IMPORTANT: Only EU AI Act and US EO thresholds are policy-defined.\n",
            "Compute milestones (10×, 100×, ~3000× US EO) are training budget forecasts.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Get growth rate for display\n",
        "slope_val = models[selected_model].params.get('slope', models[selected_model].params.get('slope_after', 0.65))\n",
        "slope_se = models[selected_model].residual_std / np.sqrt(np.sum((X - X.mean())**2))\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"NOTEBOOK COMPLETE\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\"\"\n",
        "Generated outputs:\n",
        "  Figures:\n",
        "    - calibrated_forecast_final.png/pdf (Main result)\n",
        "    - crossing_timeline_final.png/pdf (Key finding)\n",
        "    - model_comparison.png/pdf (Methodology)\n",
        "    - calibration_analysis.png/pdf (Validation)\n",
        "    - selected_model_backtest.png/pdf (Diagnostics)\n",
        "    - eci_forecast.png/pdf (Capability forecast)\n",
        "    - robustness_checks.png/pdf (Sensitivity)\n",
        "    \n",
        "  Data:\n",
        "    - submission_results.json (All results)\n",
        "    - crossing_estimates_final.csv\n",
        "    - model_comparison.csv\n",
        "    - forecast_results_final.csv\n",
        "\n",
        "Key findings:\n",
        "  1. Frontier compute growth: ~{10**slope_val:.2f}x/year (95% CI: [{10**(slope_val - 1.96*slope_se):.2f}x, {10**(slope_val + 1.96*slope_se):.2f}x])\n",
        "  2. EU AI Act threshold (10²⁵): Already crossed\n",
        "  3. US EO threshold (10²⁶): Already crossed  \n",
        "  4. 10× US EO (10²⁷): Expected ~2026\n",
        "  5. 100× US EO (10²⁸): Expected ~2027\n",
        "  6. ~3000× US EO (10²⁹·⁵): Expected ~2028-2029\n",
        "\n",
        "IMPORTANT: Only EU AI Act and US EO thresholds are policy-defined.\n",
        "Compute milestones (10×, 100×, ~3000× US EO) are training budget forecasts.\n",
        "\"\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
